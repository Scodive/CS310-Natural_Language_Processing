{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 11: Explore Natural Language Generation\n",
    "\n",
    "In this lab, we will practice using pre-trained transformer-based language models for natural language generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Explore Pretrained GPT-2 Model\n",
    "\n",
    "In this task, you will explore the GPT-2 model using the `transformers` library.\n",
    "\n",
    "Just like in the previous lab, you will need to download the pretrained model and unzip it to `./gpt2zh`. \n",
    "\n",
    "Note that this is not the original version of GPT-2 provided by OpenAI (https://huggingface.co/openai-community/gpt2), but a fine-tuned version for Chinese text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 21128\n",
      "special token [SEP]: 102\n",
      "special token [CLS]: 101\n",
      "special token [PAD]: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"./gpt2zh\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"./gpt2zh\")\n",
    "# Evaluation mode\n",
    "gpt2_model = gpt2_model.half()\n",
    "\n",
    "print('vocab size:', gpt2_tokenizer.vocab_size)\n",
    "print(f'special token {gpt2_tokenizer.sep_token}:', gpt2_tokenizer.sep_token_id)\n",
    "print(f'special token {gpt2_tokenizer.cls_token}:', gpt2_tokenizer.cls_token_id)\n",
    "print(f'special token {gpt2_tokenizer.pad_token}:', gpt2_tokenizer.pad_token_id)\n",
    "\n",
    "# Use [SEP] as end-of-sentence token\n",
    "gpt2_model.config.eos_token_id = gpt2_tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer can return the token IDs and the attention mask that indicates which tokens are padding tokens (`1` for real tokens, `0` for padding tokens).\n",
    "\n",
    "Since we only have one sentence in the \"batch\", there is no padding used, and thus no `0` in the attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: tensor([[ 101, 2110, 5445, 3198,  739,  722, 8024,  679,  771, 6432,  725, 8013,\n",
      "          102]])\n",
      "input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "input tokens: ['[CLS]', '学', '而', '时', '习', '之', '，', '不', '亦', '说', '乎', '！', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_text = '学而时习之，不亦说乎！'\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print('input ids:', input_encoded['input_ids'])\n",
    "print('input attention mask:', input_encoded['attention_mask'])\n",
    "\n",
    "# Map token ids back to tokens\n",
    "print('input tokens:', gpt2_tokenizer.convert_ids_to_tokens(input_encoded['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to directly use the `generate` method to generate some sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子 曰 ： 人 民 ， 我 为 人 民 ， 我 为 人 民 ， 我 为 人 民\n",
      "子 曰 ： 人 物 有 情 ， 所 以 有 情 。 （ 《 世 纪 英 雄 》\n",
      "子 曰 ： 人 之 道 ， 君 之 道 也 ， 子 之 道 也 。 君 之 道\n",
      "子 曰 ： 人 不 可 以 不 可 以 不 可 以 不 可 以 不 可 以 不\n",
      "子 曰 ： 人 生 ， 我 们 在 生 命 的 路 上 ， 在 生 命 的 路\n"
     ]
    }
   ],
   "source": [
    "input_text = \"子曰：人\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "n_outputs = 5\n",
    "\n",
    "output = gpt2_model.generate(**input_encoded, \n",
    "                                 max_length=20, \n",
    "                                 num_return_sequences=n_outputs,\n",
    "                                 do_sample=True, \n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95, \n",
    "                                 temperature=0.7,\n",
    "                                 pad_token_id=0,\n",
    "                                 )\n",
    "# print(type(output))\n",
    "# print(output.shape)\n",
    "\n",
    "for i in range(n_outputs):\n",
    "    output_text = gpt2_tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the generation is far from perfect. It still has good chances to produce a lot of repetitions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Implement Top-k Sampling Manually\n",
    "\n",
    "Let's first try greedy search, i.e., top-1 sampling.\n",
    "\n",
    "*Hint*: Call `argmax()` on the logits; Use the `convert_ids_to_tokens()` method to convert the token ids to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: 4\n",
      "torch.Size([1, 4, 21128])\n",
      "预\n"
     ]
    }
   ],
   "source": [
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "print('input size:', input_encoded.input_ids.shape[1])\n",
    "\n",
    "output = gpt2_model(input_encoded.input_ids, \n",
    "                   attention_mask=input_encoded.attention_mask)\n",
    "logits = output.logits\n",
    "print(logits.shape)\n",
    "\n",
    "# Get the probability distribution predicted at the last token's position\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get the most likely token id from this distribution\n",
    "most_likely_token_id = torch.argmax(last_token_logits).item()  # 使用 .item() 获取标量值\n",
    "\n",
    "# Convert the token id to a token\n",
    "most_likely_token = gpt2_tokenizer.convert_ids_to_tokens([most_likely_token_id])[0]  # 将标量包装成列表\n",
    "print(most_likely_token)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# input size: 4\n",
    "# torch.Size([1, 4, 21128])\n",
    "# 预"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with the above code, you can now implement the full generation loop: at each iteration, you select the most likely token and append it to the end input, and then feed the new input to the model for predicting the next token. \n",
    "\n",
    "The loop continues until `max_gen_len` is reached, or a `\"[SEP]\"` token is generated.\n",
    "\n",
    "**Note**: \n",
    "- Use `torch.cat` to append elements to input IDs\n",
    "- The `attn_mask` also needs be updated at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气预报：今天白天，我市阴天有小雨，气温：小雨转多云，气温：小雨转多云，气温：小雨转多云，气温：小雨转多"
     ]
    }
   ],
   "source": [
    "max_gen_len = 50\n",
    "\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "count = 0\n",
    "while count < max_gen_len:\n",
    "    output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "    logits = output.logits\n",
    "\n",
    "    # Get the last token's logits\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Get the most likely token id\n",
    "    sampled_token_id = torch.argmax(last_token_logits)\n",
    "    \n",
    "    if sampled_token_id == gpt2_tokenizer.sep_token_id:\n",
    "        break\n",
    "\n",
    "    # Append the sampled token id to the input\n",
    "    input_ids = torch.cat([input_ids, sampled_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "    # Increment the attention mask\n",
    "    attn_mask = torch.cat([attn_mask, torch.ones(1, 1, dtype=attn_mask.dtype)], dim=1)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "\n",
    "# Test\n",
    "SPECIAL_TOKEN_IDS = set([gpt2_tokenizer.sep_token_id, \n",
    "                         gpt2_tokenizer.cls_token_id, \n",
    "                         gpt2_tokenizer.pad_token_id,\n",
    "                         100]) # 100 for [UNK]\n",
    "\n",
    "# Decode the generated tokens ids\n",
    "for i in range(input_ids.shape[1]):\n",
    "    tok_id = input_ids[0, i].item()\n",
    "    # Skip the special tokens\n",
    "    if tok_id not in SPECIAL_TOKEN_IDS:\n",
    "        print(gpt2_tokenizer.convert_ids_to_tokens(input_ids[0, i].item()), end='')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# 今天天气预报：今天白天，我市阴天有小雨，气温：小雨转多云，气温：小雨转多云，气温：小雨转多云，气温：小雨转多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, greedy search results in very repetitive text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a `top-k` sampling algorithm.\n",
    "\n",
    "The idea is to **uniformly** sample from top-k most likely next tokens. PyTorch tensor provides a `topk` method to get the top-k values and indices. \n",
    "\n",
    "In the following example, you can check the **top 5** most likely words following the sentence \"今天天气\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's integrate the top-k sampling algorithm into the generation process. The uniform sampling can be implemented using `random.choices` among the top-k indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_uniform(input_text, k=5, max_gen_len=50):\n",
    "    '''\n",
    "    Generate tokens from the top-k logits, and yield the sampled token id.\n",
    "    Tokens are sampled from a naive uniform distribution.\n",
    "    '''\n",
    "    input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = input_encoded.input_ids\n",
    "    attn_mask = input_encoded.attention_mask\n",
    "\n",
    "    count = 0\n",
    "    while count < max_gen_len:\n",
    "        output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "        logits = output.logits\n",
    "\n",
    "        # Get the last token's logits\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Get top-k logits and indices\n",
    "        topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "        \n",
    "        # Sample uniformly from top-k indices\n",
    "        sampled_token_id = topk_indices[random.randint(0, k-1)].item()  # 添加 .item()\n",
    "        \n",
    "        yield sampled_token_id\n",
    "        if sampled_token_id == gpt2_tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the sampled token id to the input\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[sampled_token_id]])], dim=1)\n",
    "        # Increment the attention mask\n",
    "        attn_mask = torch.cat([attn_mask, torch.ones(1, 1, dtype=attn_mask.dtype)], dim=1)\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气寒意丝件可可兰还不上？真要是看来可我得加温不到？又，为的要知晓他家一般来这有钱了没准一来想让吃，想想"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"今天天气\"\n",
    "print(input_text, end='')\n",
    "for tok_id in generate_topk_uniform(input_text, k=50):\n",
    "    if tok_id not in SPECIAL_TOKEN_IDS:\n",
    "        print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子曰：人间所述难于自白不用说你在家和母。何因他不喜那般于物易自是何愁天时日落花下我只见花时开当晚还下面笑如雪"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"子曰：人\"\n",
    "print(input_text, end='')\n",
    "for tok_id in generate_topk_uniform(input_text, k=50):\n",
    "    if tok_id not in SPECIAL_TOKEN_IDS:\n",
    "        print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8906, 7.8555, 7.5859, 7.3477, 7.3047], dtype=torch.float16,\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([7564, 2523,  679, 1962, 6820])\n",
      "预 很 不 好 还 "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "input_text = \"今天天气\"\n",
    "input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = input_encoded.input_ids\n",
    "attn_mask = input_encoded.attention_mask\n",
    "\n",
    "output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "logits = output.logits\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Get the last token's logits\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get top-k logits and indices\n",
    "topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(topk_logits)\n",
    "print(topk_indices)\n",
    "\n",
    "for i in range(k):\n",
    "    tok_id = topk_indices[i].item()\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end=' ')\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# tensor([7.8924, 7.8550, 7.5893, 7.3502, 7.3069], grad_fn=<TopkBackward0>)\n",
    "# tensor([7564, 2523,  679, 1962, 6820])\n",
    "# 预 很 不 好 还 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that although the above uniform top-k sampling solves repetition issue, it will however produce *extremely incoherent* text. We can remedy this by using a proportional sampling instead of uniform sampling.\n",
    "\n",
    "There are plenty of different ways to implement proportionaly sampling. You can either:\n",
    "- Create list of cumulative relative probabilities of the top k tokens. For instance, if the relative probabilities of $k=5$ tokens are $0.1$, $0.2$, $0.5$, $0.1$, and $0.1$, then you cumulative probability list is `cum_prob = [0.1, 0.3, 0.8, 0.9, 1.0]`. \n",
    "- Then you draw a random number $r$ from the unifrom distribution $[0,1]$ by `random.random()`, and you decide which token is sampled by telling which bin of `cum_prob` that $r$ falls into.\n",
    "- Or instead, you use the `torch.multinomial()` function to accomplish similar sampling. *Note* the input weight provided to `torch.multinomial` should be the relative probabilities of the top $k$ tokens, which can be obtained from applying softmax to the logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_proportion(input_text, k=50, max_gen_len=50):\n",
    "    '''\n",
    "    Generate tokens from the top-k logits, and yield the sampled token id.\n",
    "    Tokens are sampled proportional to their logits.\n",
    "    '''\n",
    "    input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = input_encoded.input_ids\n",
    "    attn_mask = input_encoded.attention_mask\n",
    "\n",
    "    count = 0\n",
    "    while count < max_gen_len:\n",
    "        output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "        logits = output.logits\n",
    "\n",
    "        # Get the last token's logits\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Get top-k logits and indices\n",
    "        topk_logits, topk_indices = torch.topk(last_token_logits, k)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        topk_probs = F.softmax(topk_logits, dim=0)\n",
    "        \n",
    "        # Sample from top-k probabilities\n",
    "        sampled_token_id = topk_indices[torch.multinomial(topk_probs, 1).item()].item()  # 添加 .item()\n",
    "        \n",
    "        yield sampled_token_id\n",
    "        if sampled_token_id == gpt2_tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the sampled token id to the input\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[sampled_token_id]])], dim=1)\n",
    "        # Increment the attention mask\n",
    "        attn_mask = torch.cat([attn_mask, torch.ones(1, 1, dtype=attn_mask.dtype)], dim=1)\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气转好，预计今天白天多云有阵雨，但是气温会比较平稳。今天是星期一，气温在22##℃左右，也许还会再##℃.3至23"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"今天天气\"\n",
    "print(input_text, end='')\n",
    "for tok_id in generate_topk_proportion(input_text, k=50):\n",
    "    if tok_id not in SPECIAL_TOKEN_IDS:\n",
    "        print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子曰：人生所有的经历都是不一样的，人生是一个有序循序渐进有结果有过程的，它不仅仅是个短暂的，如果需要时间的话"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"子曰：人\"\n",
    "print(input_text, end='')\n",
    "for tok_id in generate_topk_proportion(input_text, k=50):\n",
    "    if tok_id not in SPECIAL_TOKEN_IDS:\n",
    "        print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the proportional sampling produces better text?\n",
    "\n",
    "Have fun sampling! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Implement Top-p Sampling\n",
    "\n",
    "Next, we will implement top-p sampling, which works in parallel to top-k sampling.\n",
    "\n",
    "In `filter_topk_topp()`, we first filter out the logits that are not in the top-k, by setting their logit values to `-float('inf')`. \n",
    "\n",
    "And then filter out the logits whose cumulative probability (as computed from the altered logits from the previous step) is greater than `p`.\n",
    "\n",
    "Note that it is possible that the first logit alone dominates the distribution, and its cumulative probability is greater than `p`. In this case, we want to keep this logit, and remove all other logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_topk_topp(logits: torch.Tensor, k=50, p=0.9) -> torch.Tensor: \n",
    "    '''\n",
    "    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    '''\n",
    "    assert logits.dim() == 1\n",
    "    logits = logits.clone()\n",
    "\n",
    "    if k > 0:\n",
    "        # Get top-k logits and indices\n",
    "        topk_logits, topk_indices = torch.topk(logits, k)\n",
    "        # Create a mask for logits to keep\n",
    "        mask = torch.zeros_like(logits, dtype=torch.bool)\n",
    "        mask[topk_indices] = True\n",
    "        # Set non-top-k logits to -inf\n",
    "        logits[~mask] = -float('Inf')\n",
    "    \n",
    "    if p > 0.0:\n",
    "        # Sort logits in descending order\n",
    "        logits_sorted, indices_sorted = torch.sort(logits, descending=True)\n",
    "        # Compute cumulative probabilities\n",
    "        probs_sorted = F.softmax(logits_sorted, dim=0)\n",
    "        cum_probs = torch.cumsum(probs_sorted, dim=0)\n",
    "        \n",
    "        # Find indices to remove\n",
    "        indices_to_remove = cum_probs > p\n",
    "        # Always keep the first token\n",
    "        indices_to_remove[0] = False\n",
    "        \n",
    "        # Set filtered logits to -inf\n",
    "        logits[indices_sorted[indices_to_remove]] = -float('Inf')\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original logits: tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "\n",
      "k=5, p=0.0: tensor([-inf, -inf, -inf, -inf, -inf, 5., 6., 7., 8., 9.])\n",
      "\n",
      "k=0, p=0.9: tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 8., 9.])\n",
      "\n",
      "k=0, p=0.9999999: tensor([-inf, 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "\n",
      "k=5, p=0.9999999: tensor([-inf, -inf, -inf, -inf, -inf, 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Test filter_topk_topp\n",
    "logits = torch.tensor(list(range(10))).float()\n",
    "print('original logits:', logits)\n",
    "\n",
    "logits2 = filter_topk_topp(logits, k=5, p=0.0)\n",
    "print('\\nk=5, p=0.0:', logits2)\n",
    "\n",
    "logits3 = filter_topk_topp(logits, k=0, p=0.9)\n",
    "print('\\nk=0, p=0.9:', logits3)\n",
    "\n",
    "logits4 = filter_topk_topp(logits, k=0, p=0.9999999)\n",
    "print('\\nk=0, p=0.9999999:', logits4)\n",
    "\n",
    "logits5 = filter_topk_topp(logits, k=5, p=0.9999999)\n",
    "print('\\nk=5, p=0.9999999:', logits5)\n",
    "\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# original logits: tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
    "# k=5, p=0.0: tensor([-inf, -inf, -inf, -inf, -inf, 5., 6., 7., 8., 9.])\n",
    "# k=0, p=0.9: tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 8., 9.])\n",
    "# k=0, p=0.9999999: tensor([-inf, 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
    "# k=5, p=0.9999999: tensor([-inf, -inf, -inf, -inf, -inf, 5., 6., 7., 8., 9.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following test, if all logits are `-inf`, then your top-p sampling is not correctly implemented. \n",
    "\n",
    "You wan to keep at least one element in the logits, whose logit value dominates the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original logits: tensor([ 0.,  1.,  4.,  9., 16., 25., 36., 49., 64., 81.])\n",
      "\n",
      "k=0, p=0.9: tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 81.])\n"
     ]
    }
   ],
   "source": [
    "logits_special = torch.tensor(np.arange(10)**2).float()\n",
    "print('original logits:', logits_special)\n",
    "\n",
    "logits6 = filter_topk_topp(logits_special, k=0, p=0.9)\n",
    "print('\\nk=0, p=0.9:', logits6)\n",
    "\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# original logits: tensor([ 0.,  1.,  4.,  9., 16., 25., 36., 49., 64., 81.])\n",
    "# k=0, p=0.9: tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 81.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we integrate the filtering to the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_topp(input_text, k=50, p=0.9, max_gen_len=20):\n",
    "    '''\n",
    "    Generate tokens from the top-k and top-p filtered logits, and yield the sampled token id.\n",
    "    '''\n",
    "    input_encoded = gpt2_tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = input_encoded.input_ids\n",
    "    attn_mask = input_encoded.attention_mask\n",
    "\n",
    "    count = 0\n",
    "    while count < max_gen_len:\n",
    "        output = gpt2_model(input_ids, attention_mask=attn_mask)\n",
    "        logits = output.logits\n",
    "\n",
    "        # Get last token logits\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Get the filtered logits\n",
    "        filtered_logits = filter_topk_topp(last_token_logits, k=k, p=p)\n",
    "        \n",
    "        # Sample from the remaining tokens\n",
    "        filtered_probs = F.softmax(filtered_logits, dim=0)\n",
    "        try:\n",
    "            sampled_index = torch.multinomial(filtered_probs, 1).item()\n",
    "        except RuntimeError:\n",
    "            raise\n",
    "\n",
    "        # Yield the sampled token id\n",
    "        yield sampled_index\n",
    "        if sampled_index == gpt2_tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the sampled token id to the input_ids, and extend the attention mask\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[sampled_index]])], dim=1)\n",
    "        attn_mask = torch.cat([attn_mask, torch.ones(1, 1, dtype=attn_mask.dtype)], dim=1)\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气预报[UNK]的字样被封杀了，而且在官方发布的文"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"今天天气\"\n",
    "print(input_text, end='')\n",
    "for tok_id in generate_topk_topp(input_text, k=50, p=0.95):\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子曰：人心不古，有圣人之德，必有子民之德。古人是"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "input_text = \"子曰：人\"\n",
    "print(input_text, end='')\n",
    "for tok_id in generate_topk_topp(input_text, k=50, p=0.95):\n",
    "    print(gpt2_tokenizer.convert_ids_to_tokens(tok_id), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
