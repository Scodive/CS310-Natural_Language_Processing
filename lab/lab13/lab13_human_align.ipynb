{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b29f68",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 13: Human Alignment\n",
    "\n",
    "In this lab, we will practice two tasks:\n",
    "- Using the code framework for training a reward model that assigns scores to pairs of sentences. \n",
    "- Getting familiar with the code framework for Direct Preference Optimization (DPO).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149117e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sco/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LlamaForCausalLM,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2573d",
   "metadata": {},
   "source": [
    "## T1. Defining Reward Model\n",
    "\n",
    "\n",
    "We will use the [LlamaForCausalLM](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaForCausalLM) model from HuggingFace, as the basis for our reward model.\n",
    "\n",
    "First, two internal forward functions are to be implemented:\n",
    "- `_forward_rm`: it takes the input ids and attention masks of a sequence (user input + response), and returns the reward scores.\n",
    "  - The reward scores are in tensor of same shape as the input ids, with **one reward score for each token**.\n",
    "  - Reward scores are calculated by calling a linear layer `self.reward_head` on the last hidden state (of the entire sequence).\n",
    "- `_forward_lmloss`: it takes the input of same format, but returns the regular language modeling loss.\n",
    "  - Logits are computed by calling `self.lm_head` on the last hidden state.\n",
    "  - The `response_ids` are used as the target for the `nn.CrossEntropyLoss()`.\n",
    "\n",
    "Then, define the `forward` function, which takes the input ids and attention masks of two sequences, and returns the combined loss.\n",
    "- Compute `reward1` on the first sequence (positve example) and `reward2` on the second sequence (negative example).\n",
    "- Calculate their difference in `logits`\n",
    "- Reward loss is computed by calling `F.binary_cross_entropy_with_logits(logits, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d6d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRewardModel(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # A linear layer to map hidden states to a scalar, as the final reward\n",
    "        self.reward_head = nn.Linear(config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def _forward_rm(self, input_ids, attention_mask, **kargs):\n",
    "        \"\"\"\n",
    "        input_ids: input token ids\n",
    "        attention_mask: attention mask\n",
    "        Return: reward scores, output from self.reward_head\n",
    "        \"\"\"\n",
    "        # Call self.model.forward()  to get the hidden states\n",
    "        output = self.model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, \n",
    "            return_dict=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "        ### START YOUR CODE ###\n",
    "        # Feed the last hidden state from output to self.reward_head to get the reward score\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "    # 通过reward_head得到奖励分数\n",
    "        rewards = self.reward_head(last_hidden_state)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return rewards \n",
    "    \n",
    "    def _forward_lmloss(self, prompt_ids, lm_attn_mask, response_ids):\n",
    "        \"\"\"\n",
    "        input_ids: input token ids\n",
    "        attention_mask: attention mask\n",
    "        Return: cross-entropy loss for language modeling\n",
    "        \"\"\" \n",
    "        # Call self.model.forward()  to get the hidden states\n",
    "        outputs = self.model.forward(\n",
    "            input_ids=prompt_ids,\n",
    "            attention_mask=lm_attn_mask,\n",
    "            return_dict=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        # 获取最后一层的隐藏状态\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        # 通过lm_head得到logits\n",
    "        logits = self.lm_head(last_hidden_state)\n",
    "        \n",
    "        # 计算交叉熵损失\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # 将logits和response_ids调整为正确的形状\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        response_ids = response_ids.view(-1)\n",
    "        loss = criterion(logits, response_ids)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def forward(self, sent1_idx, attention_mask_1, sent2_idx, attention_mask_2, labels, prompt_ids, lm_attn_mask, response_ids, **kargs):\n",
    "        \"\"\"\n",
    "        sent1_idx: User input ids + positive output ids\n",
    "        attention_mask_1: Attention mask for sent1_idx\n",
    "        sent2_idx: User input ids + negative output ids\n",
    "        attention_mask_2: Attention mask for sent2_idx\n",
    "\n",
    "        labels: Positive output ids (all zeros)\n",
    "\n",
    "        prompt_ids: User input ids + positive output ids\n",
    "        lm_attn_mask: Attention mask for prompt_ids\n",
    "        response_ids: Target ids for calculating cross-entropy loss\n",
    "        \"\"\"\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "    # 计算正例的奖励分数\n",
    "        reward0 = self._forward_rm(sent1_idx, attention_mask_1)\n",
    "        # 计算负例的奖励分数\n",
    "        reward1 = self._forward_rm(sent2_idx, attention_mask_2)\n",
    "        # 计算奖励差异\n",
    "        logits = reward0 - reward1\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        # Compute the reward modeling loss\n",
    "        rm_loss = F.binary_cross_entropy_with_logits(logits, labels.to(logits.dtype), reduction=\"mean\")\n",
    "\n",
    "        # Compute the language modeling loss \n",
    "        lm_loss = self._forward_lmloss(prompt_ids, lm_attn_mask, response_ids)\n",
    "\n",
    "        # Final loss\n",
    "        loss = rm_loss + lm_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type qwen to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 8 files:   0%|          | 0/8 [02:35<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "#model = LlamaRewardModel.from_pretrained('/Users/xy/models/llama-2-7b-hf')\n",
    "#model = LlamaRewardModel.from_pretrained('/Users/xy/models/qwen2_5-7b/')\n",
    "model = LlamaRewardModel.from_pretrained(\n",
    "    \"Qwen/Qwen-7B\",\n",
    "    revision=\"main\",  # 指定版本\n",
    "    trust_remote_code=True  # 信任远程代码\n",
    ")\n",
    "# You expect to see the model correctly initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f020bf",
   "metadata": {},
   "source": [
    "## T2. Load Preference Data\n",
    "\n",
    "We will load the preference dataset from `Anthropic/hh-rlhf` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb3638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from transformers.hf_argparser import HfArg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242a70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arguments(TrainingArguments):\n",
    "    model_name_or_path: str = HfArg(\n",
    "        default=None, # The path to your model\n",
    "        help=\"The model name or path\"\n",
    "    )\n",
    "    \n",
    "    # Preference dataset\n",
    "    data_path: str = HfArg(\n",
    "        default='./hh-rlhf', # The path to the preference dataset\n",
    "        help=\"The path of preference dataset, e.g., `Anthropic/hh-rlhf`\",\n",
    "    )\n",
    "\n",
    "    model_max_length: int = HfArg(default=512, help=\"Maximum sequence length.\")\n",
    "\n",
    "    bf16: bool = HfArg(\n",
    "        default=True,\n",
    "        help=\"Whether to use bf16 (mixed) precision instead of 32-bit.\",\n",
    "    )\n",
    "\n",
    "    # Hyper-parameters for DPO loss\n",
    "    beta: float = HfArg(\n",
    "        default=0.1,\n",
    "        help=\"The beta factor in DPO loss.\"\n",
    "        \"Higher beta means less divergence from the initial policy.\",\n",
    "    )\n",
    "\n",
    "    output_dir: str = HfArg(\n",
    "        default=\"output\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad62a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "./hh-rlhf\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "args = Arguments()\n",
    "print(args.model_name_or_path)\n",
    "print(args.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7db53",
   "metadata": {},
   "source": [
    "The following function prepares the preference dataset in a user-friendly view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ace96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(split, data_path):\n",
    "    dataset = load_dataset(split=split, path=data_path)\n",
    "\n",
    "    def split_prompt_and_responses_hh(sample):\n",
    "        search_term = \"\\n\\nAssistant:\"\n",
    "        search_term_idx = sample[\"chosen\"].rfind(search_term)\n",
    "        assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "        prompt = sample[\"chosen\"][:search_term_idx + len(search_term)]\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt):],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt):],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5c85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      " I haven't even thought about it.\n",
      " Ass.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "train_dataset = get_data('train', args.data_path)\n",
    "\n",
    "prompt = train_dataset[0]['prompt']\n",
    "chosen = train_dataset[0]['chosen']\n",
    "rejected = train_dataset[0]['rejected']\n",
    "print(prompt[:45])\n",
    "print(chosen)\n",
    "print(rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493a453",
   "metadata": {},
   "source": [
    "Now, load tokenizer and tokenize some sample data.\n",
    "\n",
    "- `sent1_encoded` is the tokenized result of `prompt + chosen` (positive example)\n",
    "- `sent2_encoded` is the tokenized result of `prompt + rejected` (negative example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61188b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False)\n",
    "\n",
    "\n",
    "sent1_encoded = tokenizer(\n",
    "    prompt + chosen,\n",
    "    truncation=True,\n",
    "    max_length=args.model_max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sent2_encoded = tokenizer(\n",
    "    prompt + rejected,\n",
    "    truncation=True,\n",
    "    max_length=args.model_max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad4b95",
   "metadata": {},
   "source": [
    "Pad two sequences (input ids and attention masks) to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875322d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_idx = sent1_encoded['input_ids']\n",
    "sent2_idx = sent2_encoded['input_ids']\n",
    "\n",
    "# Pad input ids\n",
    "max_len = max(sent1_idx.shape[1], sent2_idx.shape[1])\n",
    "sent1_idx = torch.nn.functional.pad(sent1_idx, (0, max_len - sent1_idx.shape[1]), value=tokenizer.pad_token_id)\n",
    "sent2_idx = torch.nn.functional.pad(sent2_idx, (0, max_len - sent2_idx.shape[1]), value=tokenizer.pad_token_id)\n",
    "\n",
    "# Pad attention masks\n",
    "sent1_attn_mask = sent1_encoded['attention_mask']\n",
    "sent2_attn_mask = sent2_encoded['attention_mask']\n",
    "sent1_attn_mask = torch.nn.functional.pad(sent1_attn_mask, (0, max_len - sent1_attn_mask.shape[1]), value=0)\n",
    "sent2_attn_mask = torch.nn.functional.pad(sent2_attn_mask, (0, max_len - sent2_attn_mask.shape[1]), value=0)\n",
    "\n",
    "print(sent1_idx.shape)\n",
    "print(sent2_idx.shape)\n",
    "print(sent1_attn_mask.shape)\n",
    "print(sent2_attn_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b47f16",
   "metadata": {},
   "source": [
    "Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d90e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'sent1_idx': sent1_idx, \n",
    "    'attention_mask_1': sent1_attn_mask, \n",
    "    'sent2_idx': sent2_idx, \n",
    "    'attention_mask_2': sent2_attn_mask, \n",
    "\n",
    "    'labels': torch.zeros_like(sent1_idx), \n",
    "\n",
    "    'prompt_ids': sent1_encoded['input_ids'], \n",
    "    'lm_attn_mask': sent1_encoded['attention_mask'], \n",
    "    'response_ids': sent1_encoded['input_ids'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ff1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**input_data)\n",
    "    print(output)\n",
    "\n",
    "# You expect to see a single loss value\n",
    "# Runtime Error is likely to because by the implementation of the internal forward functions\n",
    "# You can use the following code to help you debug\n",
    "# r1 = model._forward_rmloss(sent1_idx, sent1_attn_mask)\n",
    "# print(r1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fb68c",
   "metadata": {},
   "source": [
    "## T3. (Optional) DPO Training\n",
    "\n",
    "You need to install the [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) library first.\n",
    "\n",
    "```bash\n",
    "pip install trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed20643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Parse arguments\n",
    "    parser = HfArgumentParser(Arguments)\n",
    "    args = parser.parse_args_into_dataclasses()[0]\n",
    "    \n",
    "    # Load policy model\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    # Load reference model\n",
    "    model_ref = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    # Freeze reference model\n",
    "    model_ref.eval()\n",
    "    for param in model_ref.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Tokenizer and data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        model_max_length=args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True,\n",
    "    )\n",
    "    train_dataset = get_data(\"train\", args.data_path)\n",
    "\n",
    "    # Training arguments\n",
    "    kwargs = dict(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    dpo_trainer = DPOTrainer(**kwargs)\n",
    "    dpo_trainer.train()\n",
    "    dpo_trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
