{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 1: Basic Text Processing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of lines:  4689\n"
     ]
    }
   ],
   "source": [
    "with open(\"三体3死神永生-刘慈欣.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "print('# of lines: ', len(raw))\n",
    "raw = ''.join(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T0. Cleaning the raw data\n",
    "\n",
    "1. Replace the special token `\\u3000` with empty string \"\".\n",
    "2. Replace consecutive newlines with just a single one.\n",
    "3. Other cleaning work you can think of.\n",
    "\n",
    "*Hint*: Use `re.sub()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理数据\n",
    "raw = re.sub('\\u3000', '', raw)  # 删除特殊空格符号\n",
    "raw = re.sub('\\n+', '\\n', raw)   # 将多个换行符替换为单个\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Count the number of Chinese tokens\n",
    "\n",
    "*Hint*: Use `re.findall()` and the range of Chinese characters in Unicode, i.e., `[\\u4e00-\\u9fa5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文字符数量： 329946\n"
     ]
    }
   ],
   "source": [
    "# 统计中文字符\n",
    "chinese_chars = re.findall('[\\u4e00-\\u9fa5]', raw)\n",
    "print('中文字符数量：', len(chinese_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Build the vocabulary for all Chinese tokens\n",
    "\n",
    "Use a Python `dict` object or instance of  `collections.Counter()` to count the frequency of each Chinese token.\n",
    "\n",
    "*Hint*: Go through the `raw` string and for each unique Chinese token, add it to the `dict` or `Counter` object with a count of 1. If the token is already in the `dict` or `Counter` object, increment its count by 1.\n",
    "\n",
    "Check the vocabulary size and print the top 20 most frequent Chinese tokens and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小： 3027\n",
      "\n",
      "最常见的20个字符：\n",
      "的: 15990\n",
      "一: 6749\n",
      "是: 4837\n",
      "在: 4748\n",
      "了: 4149\n",
      "有: 3656\n",
      "这: 3532\n",
      "个: 3458\n",
      "不: 3117\n",
      "人: 2988\n",
      "中: 2649\n",
      "到: 2632\n",
      "他: 2354\n",
      "上: 2194\n",
      "们: 2164\n",
      "时: 2076\n",
      "心: 2007\n",
      "地: 1953\n",
      "大: 1938\n",
      "来: 1855\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "vocab = collections.Counter()\n",
    "\n",
    "# 构建词汇表\n",
    "chinese_chars = re.findall('[\\u4e00-\\u9fa5]', raw)\n",
    "for char in chinese_chars:\n",
    "    vocab[char] += 1\n",
    "\n",
    "print('词汇表大小：', len(vocab))\n",
    "print('\\n最常见的20个字符：')\n",
    "for char, count in vocab.most_common(20):\n",
    "    print(f'{char}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Sentence segmentation\n",
    "\n",
    "Estimate the number of sentences in the `raw` string by separating the sentences with the delimiter punctuations, such as  `。`, `？`, `！` etc.\n",
    "\n",
    "*Hint*: Use `re.split()` and the correct regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量： 22855\n"
     ]
    }
   ],
   "source": [
    "# 分割句子\n",
    "# 添加更多的句子结束符号：句号(。)、感叹号(！)、问号(？)、省略号(...)、破折号(——)、分号(；)以及空格\n",
    "sentences = re.split(r'[。！？…；]|\\s+|\\.{3,}|—{2,}', raw)\n",
    "print('句子数量：', len(sentences))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences obtained with `re.split()` do not contain the delimiter punctuations. What if we want to keep the delimiter punctuations in the sentences?\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量： 22855\n",
      "带标点的句子数量： 11750\n"
     ]
    }
   ],
   "source": [
    "# 分割句子\n",
    "# 添加更多的句子结束符号：句号(。)、感叹号(！)、问号(？)、省略号(...)、破折号(——)、分号(；)以及空格\n",
    "sentences = re.split(r'[。！？…；]|\\s+|\\.{3,}|—{2,}', raw)\n",
    "print('句子数量：', len(sentences))\n",
    "\n",
    "# 保留标点的句子分割\n",
    "sentences_with_punct = re.findall(r'[^。！？…；\\s]+(?:[。！？…；]|\\s+|\\.{3,}|—{2,})', raw)\n",
    "print('带标点的句子数量：', len(sentences_with_punct))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Count consecutive English and number tokens\n",
    "\n",
    "Estimate the number of consecutive English and number tokens in the `raw` string. Build a vocabulary for them and count their frequency.\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression. Use similar method as in T2 to build the vocabulary and count the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文和数字词汇表大小： 172\n",
      "\n",
      "最常见的20个英文/数字标记：\n",
      "AA: 338\n",
      "A: 68\n",
      "I: 66\n",
      "PIA: 45\n",
      "PDC: 35\n",
      "Ice: 34\n",
      "1: 30\n",
      "IDC: 28\n",
      "DX3906: 27\n",
      "5: 26\n",
      "0: 22\n",
      "Way: 20\n",
      "647: 19\n",
      "7: 19\n",
      "3: 15\n",
      "16: 14\n",
      "11: 13\n",
      "4: 12\n",
      "2: 9\n",
      "21: 8\n"
     ]
    }
   ],
   "source": [
    "# 统计英文和数字\n",
    "en_num_tokens = re.findall('[a-zA-Z0-9]+', raw)\n",
    "en_num_vocab = collections.Counter(en_num_tokens)\n",
    "\n",
    "print('英文和数字词汇表大小：', len(en_num_vocab))\n",
    "print('\\n最常见的20个英文/数字标记：')\n",
    "for token, count in en_num_vocab.most_common(20):\n",
    "    print(f'{token}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5. Mix of patterns\n",
    "\n",
    "There are two characters whose names are \"艾AA\" and \"程心\". Find all sentences where \"艾AA\" and \"程心\" appear together. Consider fullnames only, that is, \"艾AA\" but not \"AA\" alone. \n",
    "\n",
    "*Hint*: You may find the lookbehind or lookahead pattern useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的句子数量： 16\n",
      "\n",
      "包含\"艾AA\"和\"程心\"的句子：\n",
      "\n",
      "1. 在程心眼中，艾AA是个像鸟一般轻灵的女孩子，充满生机地围着她飞来飞去\n",
      "\n",
      "2. 程心听到有人叫自己的名字，转身一看，竟是艾AA正向这里跑过来\n",
      "\n",
      "3. 程心让艾AA在原地等着自己，但AA坚持要随程心去，只好让她上了车\n",
      "\n",
      "4. 程心和艾AA是随最早的一批移民来到澳大利亚的\n",
      "\n",
      "5. 艾AA说程心的眼睛比以前更明亮更美丽了，也许她没有说谎\n",
      "\n",
      "6. ”坐在程心旁边的艾AA大叫起来，引来众人不满的侧目\n",
      "\n",
      "7. 这天，艾AA来找程心\n",
      "\n",
      "8. 是艾AA建议程心报名参加试验的，她认为这是为星环公司参与掩体工程而树立公众形象的一次极佳的免费广告，同时，她和程心都清楚试验是经过严密策划的，只是看上去刺激，基本没什么危险\n",
      "\n",
      "9. 在返回的途中，当太空艇与地球的距离缩小到三十万千米以内、通信基本没有延时时，程心给艾AA打电话，告诉了她与维德会面的事\n",
      "\n",
      "10. 与此同时，程心和艾AA进入冬眠\n",
      "\n",
      "11. 程心到亚洲一号的冬眠中心唤醒了冬眠中的艾AA，两人回到了地球\n",
      "\n",
      "12. 程心现在身处的世界是一个白色的球形空间，她看到艾AA飘浮在附近，和她一样身穿冬眠时的紧身服，头发湿漉漉的，四肢无力地摊开，显然也是刚刚醒来\n",
      "\n",
      "13. 对此程心感到很欣慰，到了新世界后，艾AA应该有一个美好的新生活了\n",
      "\n",
      "14. 程心想到了云天明和艾AA，他们在地面上，应该是安全的，但现在双方已经无法联系，她甚至都没能和他说上一句话\n",
      "\n",
      "15. 程心和关一帆再次拥抱在一起，他们都为艾AA和云天明流下了欣慰的泪水，幸福地感受着那两个人在十八万个世纪前的幸福，在这种幸福中，他们绝望的心灵变得无比宁静了\n",
      "\n",
      "16. 智子的话让程心想到了云天明和艾AA刻在岩石上的字，但关一帆想到的更多，他注意到了智子提到的一个词：田园时代\n"
     ]
    }
   ],
   "source": [
    "# 先分割成句子\n",
    "sentences_with_punct = re.findall(r'[^。！？…；\\s]+(?:[。！？…；]|\\s+|\\.{3,}|—{2,})', raw)\n",
    "\n",
    "# 在每个句子中查找包含\"艾AA\"和\"程心\"的句子\n",
    "matched_sentences = []\n",
    "for sentence in sentences:\n",
    "    if \"艾AA\" in sentence and \"程心\" in sentence:\n",
    "        matched_sentences.append(sentence)\n",
    "\n",
    "print('找到的句子数量：', len(matched_sentences))\n",
    "print('\\n包含\"艾AA\"和\"程心\"的句子：')\n",
    "for i, sent in enumerate(matched_sentences, 1):\n",
    "    print(f'\\n{i}. {sent}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
