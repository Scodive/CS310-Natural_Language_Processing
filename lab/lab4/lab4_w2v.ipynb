{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 4 (part 2): Data preparation for implementing word2vec\n",
    "\n",
    "skipgram architecture and negative sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pprint import pprint\n",
    "from utils import CorpusReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n"
     ]
    }
   ],
   "source": [
    "# We set min_count=1 to include all words in the corpus\n",
    "corpus = CorpusReader(inputFileName=\"lunyu_20chapters.txt\", min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "子\n",
      "1352\n"
     ]
    }
   ],
   "source": [
    "print(corpus.word2id[\"子\"])\n",
    "print(corpus.id2word[1])\n",
    "print(len(corpus.id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient way for negative sampling\n",
    "\n",
    "In `utils.CorpusReader` class, we have implemented a method `initTableNegatives`. It creates a list of words (`self.negatives`) with a size of 1e8. This size is set a large value so that it scales up to very large corpus. \n",
    "\n",
    "The list contains the index of each word in the vocabulary, whose probability is proportional to the power of 0.75 of the word's original frequency count. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simulation of how initTableNegatives works\n",
    "# The impl. in utils.py is a bit different, but the idea is the same\n",
    "word_frequency = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n",
    "\n",
    "# the scaled sum of frequencies Z = 1**0.75 + 2**0.75 + 3**0.75 + 4**0.75 = 7.7897270\n",
    "# then the scaled probability of a = 1**0.75 / Z = 0.12837420128374202\n",
    "# the scaled probability of b = 2**0.75 / Z = 0.21589881215898812\n",
    "# the scaled probability of c = 3**0.75 / Z = 0.29262990292629903\n",
    "# the scaled probability of d = 4**0.75 / Z = 0.3630970836309708\n",
    "\n",
    "def initTableNegatives():\n",
    "    pow_frequency = np.array(list(word_frequency.values())) ** 0.75\n",
    "    words_pow = sum(pow_frequency)\n",
    "    ratio = pow_frequency / words_pow\n",
    "    count = np.round(ratio * CorpusReader.NEGATIVE_TABLE_SIZE)\n",
    "    negatives = []\n",
    "    for wid, c in enumerate(count):\n",
    "        negatives += [wid] * int(c)\n",
    "    negatives = np.array(negatives)\n",
    "    np.random.shuffle(negatives)\n",
    "    return negatives\n",
    "\n",
    "negatives = initTableNegatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999999\n",
      "{0, 1, 2, 3}\n",
      "0.12837420128374202\n",
      "0.21589881215898812\n",
      "0.29262990292629903\n",
      "0.3630970836309708\n"
     ]
    }
   ],
   "source": [
    "print(len(negatives))\n",
    "print(set(negatives)) # the word indices: a -> 0, b -> 1, c -> 2, d -> 3\n",
    "print(np.sum(negatives == 0) / len(negatives)) # should be the scaled probability of a\n",
    "print(np.sum(negatives == 1) / len(negatives)) # should be the scaled probability of b\n",
    "print(np.sum(negatives == 2) / len(negatives)) # should be the scaled probability of c\n",
    "print(np.sum(negatives == 3) / len(negatives)) # should be the scaled probability of d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `getNegatives` method returns the negative samples for a target word. The idea is to chop off a segment of given `size` from the `negatives` list. \n",
    "\n",
    "If the segment contains the target word, it is discarded and a new segment is taken. This is done to avoid the target word itself to be sampled as a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 87,  40, 528, 243, 275])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test some examples\n",
    "corpus.getNegatives(target=1, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Generate data for training\n",
    "\n",
    "Now we are going to implement the sliding window to generate center, outside, and negative words for each position in a sentence.\n",
    "\n",
    "- It takes a list of words as input and go through each word as a center word.\n",
    "- For each center word, both the left and right `window_size` words are considered as outside words. This number is smaller near the two ends of the sentence.\n",
    "- Call `corpus.getNegatives` to get negative samples for each center word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(words: List[str], window_size: int, k: int, corpus: CorpusReader):\n",
    "    \"\"\" Generate the training data for word2vec skip-gram model\n",
    "    Args:\n",
    "        text: the input text\n",
    "        window_size: the size of the context window\n",
    "        k: the number of negative samples\n",
    "        corpus: the corpus object, providing utilities such as word2id, getNegatives, etc.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    word_ids = [corpus.word2id[word] for word in words]\n",
    "    \n",
    "    for i, center in enumerate(word_ids):\n",
    "        # 对每个中心词获取上下文窗口内的词\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(word_ids), i + window_size + 1)\n",
    "        \n",
    "        outside_words = word_ids[start:i] + word_ids[i+1:end]\n",
    "        neg_samples = corpus.getNegatives(center, k)\n",
    "        \n",
    "        for outside in outside_words:\n",
    "            yield center, outside, neg_samples\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['学', '而', '时', '习', '之']\n",
      "word ids: [46, 8, 224, 544, 5]\n",
      "\n",
      "When window size is 3, for center word 学 -> 46\n",
      "the outside words are: \n",
      "而 -> 8\n",
      "时 -> 224\n",
      "习 -> 544\n",
      "\n",
      "output from generate_data:\n",
      "[(46, 8, array([ 29,  63, 210, 256,   4])),\n",
      " (46, 224, array([ 29,  63, 210, 256,   4])),\n",
      " (46, 544, array([ 29,  63, 210, 256,   4]))]\n"
     ]
    }
   ],
   "source": [
    "# Test generate_data\n",
    "text = \"学而时习之\"\n",
    "words = list(text)\n",
    "print('words:', words)\n",
    "print('word ids:', [corpus.word2id[word] for word in words])\n",
    "\n",
    "# first center word is 学\n",
    "print()\n",
    "print(f'When window size is 3, for center word 学 -> {corpus.word2id[\"学\"]}')\n",
    "print(f'the outside words are: ')\n",
    "print(f'而 -> {corpus.word2id[\"而\"]}')\n",
    "print(f'时 -> {corpus.word2id[\"时\"]}')\n",
    "print(f'习 -> {corpus.word2id[\"习\"]}')\n",
    "\n",
    "print()\n",
    "print('output from generate_data:')\n",
    "data = list(generate_data(list(text), window_size=3, k=5, corpus=corpus))\n",
    "pprint(data[:3])\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# words: ['学', '而', '时', '习', '之']\n",
    "# word ids: [46, 8, 224, 544, 5]\n",
    "\n",
    "# When window size is 3, for center word 学 -> 46\n",
    "# the outside words are: \n",
    "# 而 -> 8\n",
    "# 时 -> 224\n",
    "# 习 -> 544\n",
    "\n",
    "# output from generate_data:\n",
    "# [(46, 8, array([354,   3, 831, 570,  27])),\n",
    "#  (46, 224, array([1077, 1095,   89,  340,   92])),\n",
    "#  (46, 544, array([ 49, 488,   4, 269,  30]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above data are not in batch. We want all center words are batched into a tensor of dimension `batch_size`; same for the outside words and negative samples.\n",
    "\n",
    "For example, in \"学而时习之\", if `batch_size` is 4, then the returned batch[0] will contain three tensors. \n",
    "- The first tensor contains center words, i.e., 3 \"学\" plus 1 \"而\" => [46, 46, 46, 8]\n",
    "- The second tensor contains the correponding outside words, i.e., \"而\", \"时\", and \"习\" for \"学\"; \"学\" for \"而\" => [8, 224, 544,  46]\n",
    "- The third tensor contains the negative samples, whose dimension is `batch_size` $\\times$ `k`\n",
    "  \n",
    "The data type of the tensors is `torch.long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: List, batch_size: int):\n",
    "    \"\"\" Group a stream into batches and yield them as torch tensors.\n",
    "    Args:\n",
    "        data: a list of tuples\n",
    "        batch_size: the batch size \n",
    "    Yields:\n",
    "        a tuple of three torch tensors: center, outside, negative\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data) # data should be long enough\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        if i > len(data) - batch_size: # if the last batch is smaller than batch_size, pad it with the first few data\n",
    "            batch = batch + data[:i + batch_size - len(data)]\n",
    "        \n",
    "        centers = []\n",
    "        outsides = []\n",
    "        negatives = []\n",
    "        \n",
    "        for center, outside, negative in batch:\n",
    "            centers.append(center)\n",
    "            outsides.append(outside)\n",
    "            negatives.append(negative)\n",
    "        \n",
    "        # 使用yield而不是return\n",
    "        yield (torch.tensor(centers, dtype=torch.long),\n",
    "               torch.tensor(outsides, dtype=torch.long),\n",
    "               torch.tensor(negatives, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([46, 46, 46,  8]), tensor([  8, 224, 544,  46]), tensor([[ 776,    0,  101,  237,   47],\n",
      "        [ 776,    0,  101,  237,   47],\n",
      "        [ 776,    0,  101,  237,   47],\n",
      "        [  59, 1282,  969,  870,   28]]))\n",
      "[(tensor([46, 46, 46,  8]), tensor([  8, 224, 544,  46]), tensor([[ 776,    0,  101,  237,   47],\n",
      "        [ 776,    0,  101,  237,   47],\n",
      "        [ 776,    0,  101,  237,   47],\n",
      "        [  59, 1282,  969,  870,   28]])), (tensor([  8,   8,   8, 224]), tensor([224, 544,   5,  46]), tensor([[  59, 1282,  969,  870,   28],\n",
      "        [  59, 1282,  969,  870,   28],\n",
      "        [  59, 1282,  969,  870,   28],\n",
      "        [   1,    7, 1051,  293,    1]])), (tensor([224, 224, 224, 544]), tensor([  8, 544,   5,  46]), tensor([[   1,    7, 1051,  293,    1],\n",
      "        [   1,    7, 1051,  293,    1],\n",
      "        [   1,    7, 1051,  293,    1],\n",
      "        [   2,  169,   14,    1,  137]])), (tensor([544, 544, 544,   5]), tensor([  8, 224,   5,   8]), tensor([[  2, 169,  14,   1, 137],\n",
      "        [  2, 169,  14,   1, 137],\n",
      "        [  2, 169,  14,   1, 137],\n",
      "        [ 28,   1, 343,   9,  90]])), (tensor([ 5,  5, 46, 46]), tensor([224, 544,   8, 224]), tensor([[ 28,   1, 343,   9,  90],\n",
      "        [ 28,   1, 343,   9,  90],\n",
      "        [776,   0, 101, 237,  47],\n",
      "        [776,   0, 101, 237,  47]]))]\n"
     ]
    }
   ],
   "source": [
    "# Test batchify\n",
    "\n",
    "text = \"学而时习之\"\n",
    "words = list(text)\n",
    "data = list(generate_data(words, window_size=3, k=5, corpus=corpus))\n",
    "\n",
    "batches = list(batchify(data, batch_size=4))\n",
    "print(batches[0])\n",
    "\n",
    "print(batches)\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# (tensor([46, 46, 46,  8]), tensor([  8, 224, 544,  46]), tensor([[  85,    3,   72,   26,   35],\n",
    "#         [   7,    1,  487,   20,    4],\n",
    "#         [  12,  227,    2,   25,  639],\n",
    "#         [ 582,  148,   15, 1203,   85]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Implement the SkipGram class\n",
    "\n",
    "`SkipGram` is a subclass of `nn.Module`. The two key components are:\n",
    "- `__init__`: initialize the embeddings\n",
    "  - Two `nn.Embedding` objects are created: `self.emb_v` for center words; `self.emb_u` for outside words and negative samples.\n",
    "  - Each `nn.Embedding` is created with `vocab_size` and `emb_dim` as input arguments. \n",
    "  - `self.emb_v` is initialized with uniform distribution; `self.emb_u` is initialized with zeros.\n",
    "- `forward`: given input tensors, return the loss of the model\n",
    "  - Takes three tensors as input: center words, outside words, and negative samples. They are the output from the previously defined `batchify` function.\n",
    "  - Compute the loss using the formula: $-\\log\\sigma(v_c \\cdot u_o) - \\sum_{k=1}^K \\log\\sigma(-v_c \\cdot u_k)$\n",
    "\n",
    "*Hint*:\n",
    "- For the $\\log\\sigma$ function, you can use `F.logsigmoid` in PyTorch. See the imported module: `import torch.nn.functional as F`\n",
    "- If the input to `F.logsigmoid` is too large, it will return 0, which is not good for training. You can use `torch.clamp` to limit the input to a certain range. For example, `torch.clamp(x, min=-10, max=10)` will limit the input to be in the range of $[-10, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_v = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.emb_u = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_size # some experience passed down from generation to generation\n",
    "        nn.init.uniform_(self.emb_v.weight.data, -initrange, initrange) # same outcome as self.emb_v.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.constant_(self.emb_u.weight.data, 0) # same outcome as self.emb_u.weight.data.zero_()\n",
    "\n",
    "    def forward(self, center, outside, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center: the center word indices (B, )\n",
    "            outside: the outside word indices (B, )\n",
    "            negative: the negative word indices (B, k)\n",
    "        \"\"\"\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # 计算正样本的得分\n",
    "        pos_score = torch.sum(v_c * u_o, dim=1)  # (B,)\n",
    "        pos_score = torch.clamp(pos_score, min=-10, max=10)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "        \n",
    "        # 计算负样本的得分\n",
    "        neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze()  # (B, k)\n",
    "        neg_score = torch.clamp(neg_score, min=-10, max=10)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)  # 对k个负样本求和\n",
    "        \n",
    "        # 总损失\n",
    "        loss = -(pos_loss + neg_loss)\n",
    "        \n",
    "        return loss     \n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.emb_v.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_size))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.1814, 4.2008, 4.2172, 4.2307, 4.2414])\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "vacob_size =len(corpus.id2word)\n",
    "emb_size = 32\n",
    "model = SkipGram(vacob_size, emb_size)\n",
    "\n",
    "weight = torch.empty(vacob_size, emb_size)\n",
    "start_value = 0.01\n",
    "for i in range(vacob_size):\n",
    "    weight[i] = start_value + i * 0.01\n",
    "\n",
    "model.emb_v.weight.data.copy_(weight)\n",
    "model.emb_u.weight.data.copy_(weight)\n",
    "\n",
    "# Test the model\n",
    "center = torch.tensor([0, 1, 2, 3, 4])\n",
    "outside = torch.tensor([0, 1, 2, 3, 4])\n",
    "negative = torch.tensor([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n",
    "with torch.no_grad():\n",
    "    loss = model(center, outside, negative)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "# tensor([4.1814, 4.2008, 4.2172, 4.2307, 4.2414])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "Epoch 1, Average Loss: 3.0877\n",
      "Epoch 2, Average Loss: 2.6403\n",
      "Epoch 3, Average Loss: 2.4554\n",
      "Epoch 4, Average Loss: 2.2916\n",
      "Epoch 5, Average Loss: 2.1518\n",
      "Epoch 6, Average Loss: 2.0441\n",
      "Epoch 7, Average Loss: 1.9662\n",
      "Epoch 8, Average Loss: 1.9093\n",
      "Epoch 9, Average Loss: 1.8679\n",
      "Epoch 10, Average Loss: 1.8379\n",
      "\n",
      "训练完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/v6wmn5j14xb7tmgnzj8yt_4c0000gn/T/ipykernel_88684/602961008.py:28: UserWarning: Glyph 23398 (\\N{CJK UNIFIED IDEOGRAPH-5B66}) missing from current font.\n",
      "  plt.savefig('lab4_embeddings.png', dpi=300, bbox_inches='tight')\n",
      "/var/folders/_4/v6wmn5j14xb7tmgnzj8yt_4c0000gn/T/ipykernel_88684/602961008.py:28: UserWarning: Glyph 20064 (\\N{CJK UNIFIED IDEOGRAPH-4E60}) missing from current font.\n",
      "  plt.savefig('lab4_embeddings.png', dpi=300, bbox_inches='tight')\n",
      "/var/folders/_4/v6wmn5j14xb7tmgnzj8yt_4c0000gn/T/ipykernel_88684/602961008.py:28: UserWarning: Glyph 26352 (\\N{CJK UNIFIED IDEOGRAPH-66F0}) missing from current font.\n",
      "  plt.savefig('lab4_embeddings.png', dpi=300, bbox_inches='tight')\n",
      "/var/folders/_4/v6wmn5j14xb7tmgnzj8yt_4c0000gn/T/ipykernel_88684/602961008.py:28: UserWarning: Glyph 23376 (\\N{CJK UNIFIED IDEOGRAPH-5B50}) missing from current font.\n",
      "  plt.savefig('lab4_embeddings.png', dpi=300, bbox_inches='tight')\n",
      "/var/folders/_4/v6wmn5j14xb7tmgnzj8yt_4c0000gn/T/ipykernel_88684/602961008.py:28: UserWarning: Glyph 20154 (\\N{CJK UNIFIED IDEOGRAPH-4EBA}) missing from current font.\n",
      "  plt.savefig('lab4_embeddings.png', dpi=300, bbox_inches='tight')\n",
      "/var/folders/_4/v6wmn5j14xb7tmgnzj8yt_4c0000gn/T/ipykernel_88684/602961008.py:28: UserWarning: Glyph 20161 (\\N{CJK UNIFIED IDEOGRAPH-4EC1}) missing from current font.\n",
      "  plt.savefig('lab4_embeddings.png', dpi=300, bbox_inches='tight')\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 23398 (\\N{CJK UNIFIED IDEOGRAPH-5B66}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 20064 (\\N{CJK UNIFIED IDEOGRAPH-4E60}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 26352 (\\N{CJK UNIFIED IDEOGRAPH-66F0}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 23376 (\\N{CJK UNIFIED IDEOGRAPH-5B50}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 20154 (\\N{CJK UNIFIED IDEOGRAPH-4EBA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 20161 (\\N{CJK UNIFIED IDEOGRAPH-4EC1}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "解释方差比: 0.0895\n",
      "词对相似度分析：\n",
      "学-习: 0.2292\n",
      "子-曰: 0.4987\n",
      "人-仁: 0.1127\n"
     ]
    }
   ],
   "source": [
    "# 在notebook最后添加以下代码\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def visualize_embeddings(model, corpus, target_words=['学', '习', '曰', '子', '人', '仁']):\n",
    "    \"\"\"可视化词嵌入\"\"\"\n",
    "    # 获取词嵌入\n",
    "    embeddings = model.emb_v.weight.detach().cpu().numpy()\n",
    "    \n",
    "    # 使用TruncatedSVD降维到2维\n",
    "    svd = TruncatedSVD(n_components=2)\n",
    "    embeddings_2d = svd.fit_transform(embeddings)\n",
    "    \n",
    "    # 创建图形\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 绘制目标词\n",
    "    for word in target_words:\n",
    "        if word in corpus.word2id:\n",
    "            idx = corpus.word2id[word]\n",
    "            x, y = embeddings_2d[idx]\n",
    "            plt.scatter(x, y, marker='o')\n",
    "            plt.annotate(word, (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title('Word Embeddings (LSA)')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('lab4_embeddings.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 返回解释方差比\n",
    "    return svd.explained_variance_ratio_.sum()\n",
    "\n",
    "def analyze_word_pairs(model, corpus, word_pairs=[('学', '习'), ('子', '曰'), ('人', '仁')]):\n",
    "    \"\"\"分析词对之间的关系\"\"\"\n",
    "    embeddings = model.emb_v.weight.detach().cpu().numpy()\n",
    "    \n",
    "    print(\"词对相似度分析：\")\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in corpus.word2id and word2 in corpus.word2id:\n",
    "            idx1 = corpus.word2id[word1]\n",
    "            idx2 = corpus.word2id[word2]\n",
    "            vec1 = embeddings[idx1]\n",
    "            vec2 = embeddings[idx2]\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "            print(f\"{word1}-{word2}: {similarity:.4f}\")\n",
    "\n",
    "# 训练模型并分析结果\n",
    "vocab_size = len(corpus.id2word)\n",
    "emb_size = 100  # 使用与新实验相同的维度\n",
    "model = SkipGram(vocab_size, emb_size)\n",
    "\n",
    "# 读取文本\n",
    "with open(\"lunyu_20chapters.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    words = list(text.replace('\\n', '').replace(' ', ''))\n",
    "\n",
    "# 生成训练数据\n",
    "data = list(generate_data(words, window_size=3, k=5, corpus=corpus))\n",
    "\n",
    "# 训练模型\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.01)\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "print(\"开始训练...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in batchify(data, batch_size):\n",
    "        center, outside, negative = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center, outside, negative)\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.mean().item()\n",
    "        batch_count += 1\n",
    "        \n",
    "    avg_loss = total_loss / batch_count\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n训练完成！\")\n",
    "\n",
    "# 可视化结果\n",
    "explained_variance = visualize_embeddings(model, corpus)\n",
    "print(f\"\\n解释方差比: {explained_variance:.4f}\")\n",
    "\n",
    "# 分析词对关系\n",
    "analyze_word_pairs(model, corpus)\n",
    "\n",
    "# 保存词向量\n",
    "model.save_embedding(corpus.id2word, \"lab4_embeddings.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
