{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 14: In-Context Learning and Prompting\n",
    "\n",
    "In this lab, we will practice some in-context learning techniques, such as few-shot learning and chain-of-thought prompting, for solving QA problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Run LLMs locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Install llama.cpp\n",
    "\n",
    "Build the [llama.cpp](https://github.com/ggml-org/llama.cpp) tool, or download the binaries from the [release page](https://github.com/ggml-org/llama.cpp/releases).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Download model\n",
    "\n",
    "We are going to download the model that is quantized and format-converted to `gguf` format.\n",
    "\n",
    "**Model option a**: \n",
    "- Using the `huggingface-cli` tool.\n",
    "- Following the tutorial here: (Qwen2.5-7B-Instruct-GGUF)[https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF]\n",
    "\n",
    "A quick command to download the model is:\n",
    "```bash\n",
    "huggingface-cli download Qwen/Qwen1.5-7B-Chat-GGUF qwen1_5-7b-chat-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "\n",
    "**Model option b**: \n",
    "- Or you can download the ChatGLM-3 model from ModelScope: https://modelscope.cn/models/ZhipuAI/chatglm3-6b/files\n",
    "  - `model.safetensors.index.json`, `config.json`, `configuration.json`\n",
    "  - `model-00001-of-00007.safetensors` to `model-00007-of-00007.safetensors`\n",
    "  - `tokenizer_config.json`, `tokenizer.model`\n",
    "Put all the files in a folder such as `./chatglm3-6b`. \n",
    "- Then use tools like [`chatglm.cpp`](https://github.com/li-plus/chatglm.cpp) to manually convert the model weights to `ggml` format.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Run model\n",
    "\n",
    "You can run the model with following command:\n",
    "\n",
    "```bash\n",
    "llama-cli -m $MODEL_PATH\n",
    "```\n",
    "\n",
    "Then you can start interacting with the model in command line. Try to solve the following problems.\n",
    " - Use zero-shot and few-shot prompting to solve the problems.\n",
    " - Add Chain-of-Thought prompt if needed.\n",
    "\n",
    "\n",
    "Try solving these problems with prompting:\n",
    "1. Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there? A: \n",
    "2. 鸡和兔在一个笼子里，共有35个头，94只脚，那么鸡有多少只，兔有多少只？\n",
    "3. Q: 242342 + 423443 = ? A: \n",
    "4. 一个人花8块钱买了一只鸡，9块钱卖掉了，然后他觉得不划算，花10块钱又买回来了，11块卖给另外一个人。问他赚了多少?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Practice few-shot prompting\n",
    "\n",
    "For this pratice, you need to first download the [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) model from HuggingFace, by running the following command:\n",
    "\n",
    "```bash\n",
    "huggingface-cli download Qwen/Qwen2.5-7B --local-dir $MODEL_PATH\n",
    "```\n",
    "\n",
    "The task set we use is [MMLU](https://huggingface.co/datasets/cais/mmlu). You need to download the zip file and extract it to the `./MMLU` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sco/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define some helper functions for constructing prompts and running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(input_list):\n",
    "    prompt = input_list[0]\n",
    "    k = len(input_list) - 2\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], input_list[j+1])\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "def format_shots(prompt_data):\n",
    "    prompt = \"\"\n",
    "    for data in prompt_data:\n",
    "        prompt += data[0]\n",
    "        k = len(data) - 2\n",
    "        for j in range(k):\n",
    "            prompt += \"\\n{}. {}\".format(choices[j], data[j+1])\n",
    "        prompt += \"\\nAnswer:\"\n",
    "        prompt += data[k+1] + \"\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(input_list, subject, prompt_data):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject)\n",
    "    )\n",
    "    prompt += format_shots(prompt_data)\n",
    "    prompt += format_example(input_list)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `inference()` function constructs the full input by prepending the few-shot examples to the `input_text`, and generate **1** token as the output, because the task modality is multiple choice question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re # For more robust parsing of the answer\n",
    "\n",
    "# Updated inference function using Ollama API with more robust answer parsing\n",
    "def inference(input_text, subject, prompt_data, ollama_model_name=\"llama3.1:latest\", ollama_api_url=\"http://localhost:11434/api/generate\"):\n",
    "    \"\"\"\n",
    "    Performs inference using the Ollama API.\n",
    "\n",
    "    Args:\n",
    "        input_text: The primary input text or question. For MMLU, this is the question and options.\n",
    "        subject: The subject of the MMLU task (used by gen_prompt).\n",
    "        prompt_data: Few-shot examples (used by gen_prompt).\n",
    "        ollama_model_name: The name of the model in Ollama (e.g., \"llama3.1:latest\").\n",
    "        ollama_api_url: The URL of the Ollama API endpoint.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (predicted_answer, full_input_prompt, confidence).\n",
    "        predicted_answer: The extracted answer ('A', 'B', 'C', 'D', or an error message).\n",
    "        full_input_prompt: The complete prompt sent to the Ollama API.\n",
    "        confidence: Always None, as confidence scores are not readily available from this API for this task.\n",
    "    \"\"\"\n",
    "    if len(prompt_data) > 0:\n",
    "        full_input_prompt = gen_prompt(input_text, subject, prompt_data)\n",
    "    else:\n",
    "        full_input_prompt = input_text\n",
    "\n",
    "    api_payload = {\n",
    "        \"model\": ollama_model_name,\n",
    "        \"prompt\": full_input_prompt,\n",
    "        \"stream\": False,\n",
    "        # Consider uncommenting and setting num_predict if you want to restrict output length\n",
    "        # \"options\": {\n",
    "        # \"temperature\": 0.7,\n",
    "        # \"num_predict\": 5 # For MMLU, a small number like 1-5 might be enough for the letter\n",
    "        # }\n",
    "    }\n",
    "\n",
    "    predicted_answer = f\"Unparsed: Error (Initial value)\" # Default if nothing found\n",
    "    conf = None\n",
    "\n",
    "    try:\n",
    "        response = requests.post(ollama_api_url, json=api_payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        ollama_response_json = response.json()\n",
    "        ollama_response_text = ollama_response_json.get(\"response\", \"\").strip()\n",
    "        \n",
    "        # Update default predicted_answer with actual response for better \"Unparsed\" message\n",
    "        predicted_answer = f\"Unparsed: {ollama_response_text[:30]}\"\n",
    "\n",
    "\n",
    "        if ollama_response_text:\n",
    "            # Pattern 1: Try to find \"X.\" or \"X \" or just \"X\" (where X is A,B,C,D) at the start of the string.\n",
    "            # Example: \"A.\", \"A is correct\"\n",
    "            match = re.match(r\"^\\s*([ABCD])(?:[.\\s]|$)\", ollama_response_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                predicted_answer = match.group(1).upper()\n",
    "            else:\n",
    "                # Pattern 2: Try to find \"The answer is X\", \"Answer: X\", \"is X\", \"choice is X\" etc., more generally.\n",
    "                # Example: \"I think the answer is A.\", \"The correct choice is: B\"\n",
    "                # This looks for common phrases indicating an answer, followed by A, B, C, or D.\n",
    "                search_pattern = r\"(?:answer(?: is)?|choice(?: is)?|option(?: is)?|is)\\s*:?\\s*([ABCD])(?:[.\\s]|$)\"\n",
    "                match_search = re.search(search_pattern, ollama_response_text, re.IGNORECASE)\n",
    "                if match_search:\n",
    "                    predicted_answer = match_search.group(1).upper()\n",
    "                else:\n",
    "                    # Pattern 3: Fallback - find the first standalone A, B, C, or D as a letter.\n",
    "                    # This is a bit more general. Example: \"... the final option is A ...\" (if A is the answer)\n",
    "                    # Looks for A, B, C, or D as a whole word (surrounded by word boundaries).\n",
    "                    # We use re.IGNORECASE here as well.\n",
    "                    match_fallback = re.search(r\"\\b([ABCD])\\b\", ollama_response_text, re.IGNORECASE)\n",
    "                    if match_fallback:\n",
    "                         predicted_answer = match_fallback.group(1).upper()\n",
    "                    # If still no match, predicted_answer remains the \"Unparsed: {snippet}\"\n",
    "        else:\n",
    "            predicted_answer = \"Error: Empty response from API\"\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"API Request timed out to {ollama_api_url}\")\n",
    "        predicted_answer = \"Error: API call timed out\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API Request failed: {e}\")\n",
    "        predicted_answer = f\"Error: API call failed ({type(e).__name__})\"\n",
    "    \n",
    "    return predicted_answer, full_input_prompt, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他在第一笔交易中赚了 11-9=2 美元\n",
      "然后他又再次赚了 11 - 10 = <<11-10=1>>1 美元\n",
      "总共他赚了 2 + 1 = <<2+1=3>>3 美元\n",
      "答案是 3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"llama3.1:latest\",\n",
    "        \"prompt\": \"一个人花8块钱买了一只鸡，9块钱卖掉了，然后他觉得不划算，花10块钱又买回来了，11块卖给另外一个人。问他赚了多少?\",\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json()[\"response\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "假设 $x$ 是鸡的数量，$y$ 是兔子的数量。因此，我们得到两个等式：\n",
      "\n",
      "$$x+y=35,\\qquad 2x+4y=94.$$如果我们从第二个方程中减去第一个方程两倍，则得到 $$2x+4y-2(x+y)=94-2\\cdot35,$$ 或者 $$(2x+4y)-(2x+2y)=94-70.$$简化后，我们得到$$2y=24。$$从这里我们可以很容易地求出 $y$ 的值，即 $\\boxed{12}$。\n",
      "\n",
      "类似地，如果我们用第二个方程除以 2，然后使用 $y$ 的值，得到 $$x+2y=\\frac{94}{2},$$$$ x+2\\cdot12=47。$$因此，我们发现 $x$ 的值是 $\\boxed{35-12=23}$。\n",
      "\n",
      "因此，鸡有 $\\boxed{23}$ 只，兔子有 $\\boxed{12}$ 只。\n",
      "最终答案是 (23, 12)。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"llama3.1:latest\",\n",
    "        \"prompt\": \"鸡和兔在一个笼子里，共有35个头，94只脚，那么鸡有多少只，兔有多少只？\",\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json()[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原来的 model_path\n",
    "# model_path = '/Users/xy/models/qwen2.5-7b'\n",
    "\n",
    "# 修改后的 model_path，使用 Llama 3.1 8B Instruct 的 HuggingFace Hub ID\n",
    "model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# tokenizer 和 model 的加载代码保持不变，但可能需要调整 tokenizer 的参数\n",
    "# 或为 model 加载添加 torch_dtype\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          use_fast=True,\n",
    "                                          # Llama 3.1 Instruct 通常建议 add_bos_token=False\n",
    "                                          # 对于 Llama 3 系列，通常不需要手动设置 unk_token, bos_token, eos_token\n",
    "                                          # AutoTokenizer 会加载推荐的配置\n",
    "                                          add_bos_token=False \n",
    "                                          )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             device_map='auto',\n",
    "                                             # 对于 Llama 3.1 这样较新的模型，建议指定 torch_dtype 以优化性能和内存\n",
    "                                             torch_dtype=torch.bfloat16 # 如果你的硬件支持 bf16\n",
    "                                             # 或者 torch.float16\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "prompt = {}\n",
    "\n",
    "with open(f\"./MMLU/MMLU_ID_test.json\",'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(f\"./MMLU/MMLU_ID_prompt.json\",'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data is organized by subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics'])\n",
      "\n",
      "['At breakfast, lunch, and dinner, Joe randomly chooses with equal '\n",
      " 'probabilities either an apple, an orange, or a banana to eat. On a given '\n",
      " 'day, what is the probability that Joe will eat at least two different kinds '\n",
      " 'of fruit?',\n",
      " '\\\\frac{7}{9}',\n",
      " '\\\\frac{8}{9}',\n",
      " '\\\\frac{5}{9}',\n",
      " '\\\\frac{9}{11}',\n",
      " 'B']\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "\n",
    "print()\n",
    "pprint(data['high_school_mathematics'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompts also come in subjects, and each subject has a list of 5 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(prompt['high_school_mathematics']))\n",
    "print(len(prompt['high_school_physics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stick to one subject, `high_school_mathematics` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'high_school_mathematics'\n",
    "data_sub = data[subject]\n",
    "prompt_sub = prompt[subject]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one input example and generate the full prompt by calling `gen_prompt()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about  high school mathematics.\n",
      "\n",
      "Joe was in charge of lights for a dance. The red light blinks every two seconds, the yellow light every three seconds, and the blue light every five seconds. If we include the very beginning and very end of the dance, how many times during a seven minute dance will all the lights come on at the same time? (Assume that all three lights blink simultaneously at the very beginning of the dance.)\n",
      "A. 3\n",
      "B. 15\n",
      "C. 6\n",
      "D. 5\n",
      "Answer:B\n",
      "\n",
      "Five thousand dollars compounded annually at an $x\\%$ interest rate takes six years to double. At the same interest rate, how many years will it take $\\$300$ to grow to $\\$9600$?\n",
      "A. 12\n",
      "B. 1\n",
      "C. 30\n",
      "D. 5\n",
      "Answer:C\n",
      "\n",
      "The variable $x$ varies directly as the square of $y$, and $y$ varies directly as the cube of $z$. If $x$ equals $-16$ when $z$ equals 2, what is the value of $x$ when $z$ equals $\\frac{1}{2}$?\n",
      "A. -1\n",
      "B. 16\n",
      "C. -\\frac{1}{256}\n",
      "D. \\frac{1}{16}\n",
      "Answer:C\n",
      "\n",
      "Simplify and write the result with a rational denominator: $$\\sqrt{\\sqrt[3]{\\sqrt{\\frac{1}{729}}}}$$\n",
      "A. \\frac{3\\sqrt{3}}{3}\n",
      "B. \\frac{1}{3}\n",
      "C. \\sqrt{3}\n",
      "D. \\frac{\\sqrt{3}}{3}\n",
      "Answer:D\n",
      "\n",
      "Ten students take a biology test and receive the following scores: 45, 55, 50, 70, 65, 80, 40, 90, 70, 85. What is the mean of the students’ test scores?\n",
      "A. 55\n",
      "B. 60\n",
      "C. 62\n",
      "D. 65\n",
      "Answer:D\n",
      "\n",
      "At breakfast, lunch, and dinner, Joe randomly chooses with equal probabilities either an apple, an orange, or a banana to eat. On a given day, what is the probability that Joe will eat at least two different kinds of fruit?\n",
      "A. \\frac{7}{9}\n",
      "B. \\frac{8}{9}\n",
      "C. \\frac{5}{9}\n",
      "D. \\frac{9}{11}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "input_text = data_sub[3]\n",
    "prompt_text = gen_prompt(input_text, subject, prompt_sub)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New call for few-shot\n",
    "output, _, conf = inference(input_text, subject, prompt_sub) # Uses default ollama_model_name=\"llama3.1:latest\"\n",
    "# 或者显式指定模型:\n",
    "# output, _, conf = inference(input_text, subject, prompt_sub, ollama_model_name=\"your-other-ollama-model:tag\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with zero-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_prompt = '''\n",
    "    At breakfast, lunch, and dinner, Joe randomly chooses with equal probabilities either an apple, an orange, or a banana to eat. On a given day, what is the probability that Joe will eat at least two different kinds of fruit?\n",
    "    A. \\frac{7}{9}\n",
    "    B. \\frac{8}{9}\n",
    "    C. \\frac{5}{9}\n",
    "    D. \\frac{9}{11}\n",
    "    Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# New call for zero-shot\n",
    "output, _, conf = inference(zs_prompt, subject, prompt_data=[]) # Uses default ollama_model_name=\"llama3.1:latest\"\n",
    "print(output)\n",
    "print(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
