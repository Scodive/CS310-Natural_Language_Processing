{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 6: LSTM for Named Entity Recognition (NER)\n",
    "\n",
    "In this lab, we practice the data and model preparation for using LSTM for the NER task. \n",
    "\n",
    "The dataset is CoNLL2003 English named entity recognition (NER). The dataset is a collection of news articles from Reuters. \n",
    "\n",
    "The dataset is annotated with four types of named entities: \n",
    "`[persons, locations, organizations, miscellaneous]`. (`miscellaneous` does not belong to the previous three types)\n",
    "\n",
    "The dataset is divided into three parts: **training**, **development**, and **testing**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from utils import Indexer, read_ner_data_from_connl, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/dev.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "EMBEDDINGS_PATH = 'data/glove.6B.100d.txt' \n",
    "# Download from https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# It includes dimension 50, 100, 200, and 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in the IOB format. \n",
    "The IOB format is a simple text chunking format that divides the text into chunks and assigns a label to each chunk. \n",
    "\n",
    "The label is a combination of two parts: \n",
    "- the type of the named entity\n",
    "- the position of the word in the named entity. \n",
    "\n",
    "The type of the named entity is one of the four types `[persons, locations, organizations, miscellaneous]`. \n",
    "\n",
    "The position of the word in the named entity is one of three positions: `B` (beginning), `I` (inside), and `O` (outside). \n",
    "\n",
    "Examples:\n",
    "- \"New\" in the named entity \"New York\" is labeled as \"B-LOC\", and \"York\" is labeled as \"I-LOC\". \n",
    "- The word \"I\" in the sentence \"I live in New York\" is labeled as \"O\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_ner_data_from_connl(TRAIN_PATH)\n",
    "dev_words, dev_tags = read_ner_data_from_connl(DEV_PATH)\n",
    "test_words, test_tags = read_ner_data_from_connl(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train words size: 203621\n",
      "dev words size: 51362\n",
      "test words size: 46435\n"
     ]
    }
   ],
   "source": [
    "print('train words size:', len(train_words))\n",
    "print('dev words size:', len(dev_words))\n",
    "print('test words size:', len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', 'Peter']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "print(train_words[:10])\n",
    "print(train_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EU', 'B-ORG'),\n",
      " ('rejects', 'O'),\n",
      " ('German', 'B-MISC'),\n",
      " ('call', 'O'),\n",
      " ('to', 'O'),\n",
      " ('boycott', 'O'),\n",
      " ('British', 'B-MISC'),\n",
      " ('lamb', 'O'),\n",
      " ('.', 'O'),\n",
      " ('Peter', 'B-PER')]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(train_words[:10], train_tags[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that each sentence ends with token '.' and tag 'O'. Between sentences there is a blank line.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Build vocabularies for both words and tags\n",
    "\n",
    "\n",
    "`utils.py` provides an `Indexer` class that can be used to convert words and tags to indices and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of word \"the\": 40\n",
      "index of tag \"O\": 2\n",
      "word with index 0: <UNKNOWN>\n",
      "word with index 100 Fischler\n",
      "tag with index 0: <UNKNOWN>\n",
      "tag with index 1: B-ORG\n"
     ]
    }
   ],
   "source": [
    "indexer_train_words = Indexer(train_words)\n",
    "indexer_train_tags = Indexer(train_tags)\n",
    "\n",
    "# Test\n",
    "print('index of word \"the\":', indexer_train_words.element_to_index('the'))\n",
    "print('index of tag \"O\":', indexer_train_tags.element_to_index('O'))\n",
    "print('word with index 0:', indexer_train_words.index_to_element(0))\n",
    "print('word with index 100', indexer_train_words.index_to_element(100))\n",
    "print('tag with index 0:', indexer_train_tags.index_to_element(0))\n",
    "print('tag with index 1:', indexer_train_tags.index_to_element(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since the train, test, and dev sets are different, we need to build the vocabularies using **ALL** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes of indexers from all data:\n",
      "30290 10\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "all_words = train_words + dev_words + test_words\n",
    "all_tags = train_tags + dev_tags + test_tags\n",
    "\n",
    "indexer_words = Indexer(all_words)\n",
    "indexer_tags = Indexer(all_tags)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('sizes of indexers from all data:')\n",
    "print(len(indexer_words), len(indexer_tags))\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# sizes of indexers from all data:\n",
    "# 30290 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Prepare data in batch\n",
    "\n",
    "What it means by a \"batch\" of data is different from Lab 5 (Language Modeling).\n",
    "\n",
    "Because the sequence boundaries are some-what difficult to determine, and the sequences are of varying lengths, for this NER lab, we use a sloppy way to create batches: Simply use a fixed size (`batch_size`) of tokens as a batch. So there is just one long sequence in each batch.\n",
    "\n",
    "`utils.py` provides a `get_batch` function that yields `(words, tags)` in specified batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches[0] sizes: 128 128\n",
      "batches[1] sizes: 128 128\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "batches = list(get_batch(train_words, train_tags, batch_size))\n",
    "\n",
    "# Test\n",
    "print('batches[0] sizes:', len(batches[0][0]), len(batches[0][1])) \n",
    "print('batches[1] sizes:', len(batches[1][0]), len(batches[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Indexer.elements_to_indices` to convert words and tags to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_ids[:10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "tags_ids[:10] [1, 2, 3, 2, 2, 2, 3, 2, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "sequence, tags = batches[0]\n",
    "\n",
    "### START YOUR CODE ###\n",
    "sequence, tags = batches[0]\n",
    "sequence_ids = indexer_words.elements_to_indices(sequence)\n",
    "tags_ids = indexer_tags.elements_to_indices(tags)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('sequence_ids[:10]', sequence_ids[:10])\n",
    "print('tags_ids[:10]', tags_ids[:10])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# sequence_ids[:10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# tags_ids[:10] [1, 2, 3, 2, 2, 2, 3, 2, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Practice LSTM module\n",
    "\n",
    "Create a LSTM unit that takes input of dimension 3 and produces hidden state of dimension 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([1, 4])\n",
      "hidden hn size: torch.Size([1, 4])\n",
      "hidden cn size: torch.Size([1, 4])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 4)\n",
    "\n",
    "# Make a sequence of length 5\n",
    "input_seq = [torch.randn(1, 3) for _ in range(5)]\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h0 = torch.randn(1, 4)\n",
    "c0 = torch.randn(1, 4)\n",
    "hidden = (h0, c0)\n",
    "\n",
    "# Run forward pass with a loop\n",
    "for input_t in input_seq:\n",
    "    out, hidden = lstm(input_t, hidden) # Note that the hidden state from the previous time step is used as input for the current time step\n",
    "\n",
    "# Test output\n",
    "print('output size:', out.size())\n",
    "print('hidden hn size:', hidden[0].size())\n",
    "print('hidden cn size:', hidden[1].size())\n",
    "print(torch.equal(out, hidden[0])) # out is just the last hidden state hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same forward pass can be done with a single call to `lstm`, providing the entire sequence at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([5, 4])\n",
      "hidden hn size: torch.Size([1, 4])\n",
      "hidden cn size: torch.Size([1, 4])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Make a sequence of length 5 in a single tensor\n",
    "input_seq2 = torch.cat(input_seq, dim=0)\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h0 = torch.randn(1, 4)\n",
    "c0 = torch.randn(1, 4)\n",
    "hidden = (h0, c0)\n",
    "\n",
    "# Run forward pass with a single call\n",
    "out, hidden = lstm(input_seq2, hidden)\n",
    "\n",
    "# Test output\n",
    "print('output size:', out.size())\n",
    "print('hidden hn size:', hidden[0].size())\n",
    "print('hidden cn size:', hidden[1].size())\n",
    "\n",
    "print(torch.equal(out, hidden[0])) # this time out != hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time `out` is a sequence of hidden states for all times steps, not just the last one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a bi-directional LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([5, 8])\n",
      "hidden hn size: torch.Size([2, 4])\n",
      "hidden cn size: torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "bilstm = nn.LSTM(3, 4, bidirectional=True)\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h0 = torch.randn(2, 4)\n",
    "c0 = torch.randn(2, 4)\n",
    "hidden = (h0, c0)\n",
    "\n",
    "# Forward pass\n",
    "out, hidden = bilstm(input_seq2, hidden)\n",
    "\n",
    "# Test output\n",
    "print('output size:', out.size())\n",
    "print('hidden hn size:', hidden[0].size())\n",
    "print('hidden cn size:', hidden[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output size becomes $2\\times4=8$ because the LSTM is bidirectional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Using LSTM for tagging\n",
    "\n",
    "Here we practice using LSTM for tagging tasks (e.g., POS, NER). \n",
    "We will not use advanced architecture like maximum entropy Markov model (MEMM), or advanced decoding strategies such as Viterbi, or beam search decoding.\n",
    "\n",
    "The model is as follows: let the input sentence be\n",
    "$w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. Also, let\n",
    "$T$ be our tag set, and $y_i$ the tag of word $w_i$.\n",
    "\n",
    "\n",
    "Denote our prediction of the tag of word $w_i$ by\n",
    "$\\hat{y}_i$.\n",
    "This is a structure prediction, model, where our output is a sequence\n",
    "$\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$.\n",
    "\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden\n",
    "state at timestep $i$ as $h_i$. Also, assign each tag a\n",
    "unique index. \n",
    "\n",
    "Then our prediction rule for $\\hat{y}_i$ is\n",
    "\n",
    "\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(U h_i + b))_j\\end{align}\n",
    "\n",
    "That is, take the log softmax of the transformation of the hidden state $h_i$,\n",
    "and the predicted tag is the tag that has the maximum log probability. \n",
    "\n",
    "Parameters $U$ and $b$ can be implemented as a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "input_tags: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "VOCAB_SIZE = len(indexer_words)\n",
    "TAGSET_SIZE = len(indexer_tags)\n",
    "\n",
    "input_sequence = train_words[:9]\n",
    "input_tags = train_tags[:9]\n",
    "\n",
    "print('input_sequence:', input_sequence)\n",
    "print('input_tags:', input_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the model.\n",
    "\n",
    "In `__init__` method, initialize `word_embeddings` with a pretrained embedding weight matrix loaded from `glove.6B.100d.txt`.\n",
    "\n",
    "For some advanced variants of model, e.g., maximum entropy Markov model (MEMM), you also need to initialize `tag_embeddings` with a random weight matrix.\n",
    "\n",
    "`forward` method takes the sequence of word indices as input and returns the log probabilities of predicted tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "         # 词嵌入层\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 加载预训练的GloVe嵌入\n",
    "        self.load_pretrained_embeddings(EMBEDDINGS_PATH, indexer_words)\n",
    "        \n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # 全连接层，注意使用双向LSTM所以hidden_dim要乘2\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings_path, indexer):\n",
    "        \"\"\"加载预训练的GloVe词向量\"\"\"\n",
    "        embeddings_dict = {}\n",
    "        with open(embeddings_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = torch.FloatTensor([float(val) for val in values[1:]])\n",
    "                embeddings_dict[word] = vector\n",
    "        \n",
    "        # 初始化嵌入矩阵\n",
    "        pretrained_weight = torch.zeros(len(indexer), self.word_embeddings.embedding_dim)\n",
    "        for word, idx in indexer.element_to_index_dict.items():\n",
    "            if word in embeddings_dict:\n",
    "                pretrained_weight[idx] = embeddings_dict[word]\n",
    "        \n",
    "        # 更新嵌入层权重\n",
    "        self.word_embeddings.weight.data.copy_(pretrained_weight)\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        ### START YOUR CODE ###\n",
    "        embeds = self.word_embeddings(sequence)  # [seq_len, embedding_dim]\n",
    "        \n",
    "        # LSTM前向传播\n",
    "        lstm_out, _ = self.lstm(embeds.unsqueeze(0))  # 添加batch维度\n",
    "        lstm_out = lstm_out.squeeze(0)  # 移除batch维度\n",
    "        \n",
    "        # 全连接层\n",
    "        logits = self.fc(lstm_out)\n",
    "        \n",
    "        # 计算log概率\n",
    "        logprobs = F.log_softmax(logits, dim=1)\n",
    "        ### END YOUR CODE ###\n",
    "        return logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and test the forward computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m### START YOUR CODE ###\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     inputs_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Convert input sequence to tensor, using indexer_words\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mLSTMTagger.__init__\u001b[0;34m(self, embedding_dim, hidden_dim, vocab_size, tagset_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dim)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 加载预训练的GloVe嵌入\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_pretrained_embeddings(EMBEDDINGS_PATH, indexer_words)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# LSTM层\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTM(embedding_dim, hidden_dim, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mLSTMTagger.load_pretrained_embeddings\u001b[0;34m(self, embeddings_path, indexer)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"加载预训练的GloVe词向量\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m embeddings_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(embeddings_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m         values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    inputs_tensor = None # Convert input sequence to tensor, using indexer_words\n",
    "    logprobs = None\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test output\n",
    "print('logprobs shape:', logprobs.shape)\n",
    "# You are expected to see the following:\n",
    "# logprobs shape: torch.Size([9, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5. Evaluation\n",
    "\n",
    "Evaluation on multiple metrics are needed. Here we practice using the provided `metrices.py` file as a helper. \n",
    "\n",
    "In `metrices.py` there is a `MetricsHandler` class, which has an `update` method that should be called for every batch during training. \n",
    "It also has a `collect` method that should be called after each epoch.  \n",
    "\n",
    "It takes a list of classes (target tags) as input, so we need to specify this arguement properly with the `indexer_tags` object or `TAGSET_SIZE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import MetricsHandler\n",
    "\n",
    "train_metrics = MetricsHandler(classes=list(range(TAGSET_SIZE)))\n",
    "val_metrics = MetricsHandler(classes=list(range(TAGSET_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an sample segment of training and evaluate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "def train_loop():\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        for sequence, tags in batches:\n",
    "            # make prediction\n",
    "            predictions = model(sequence)\n",
    "            train_metrics.update(predictions, tags) # update() method takes the predictions and the ground truth tags as inputs\n",
    "\n",
    "        train_metrics.collect()\n",
    "\n",
    "        # print training metrics\n",
    "        for metric in train_metrics.metrics_dict.keys():\n",
    "                print(f\"{metric} - {train_metrics.metrics_dict[metric][-1]}\")\n",
    "        print()\n",
    "\n",
    "def evaluate_loop():\n",
    "    \"\"\"\n",
    "    Evaluation loop\n",
    "    \"\"\"\n",
    "    val_batches = get_batch(dev_words, dev_tags)\n",
    "    for sequence, tags in val_batches:\n",
    "        # make prediction\n",
    "        predictions = model(sequence)\n",
    "        val_metrics.update(predictions, tags)\n",
    "\n",
    "    val_metrics.collect()\n",
    "\n",
    "    # print validation metrics\n",
    "    for metric in val_metrics.metrics_dict.keys():\n",
    "        print(f\"{metric} - {val_metrics.metrics_dict[metric][-1]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
