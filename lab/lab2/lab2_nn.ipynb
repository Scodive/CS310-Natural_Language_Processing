{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 2: Neural Text Classification\n",
    "\n",
    "In this lab, we will practice building a neural network for text classification. \n",
    "\n",
    "The tutorial code is adopted from the official PyTorch tutorial: *Text classification with the torchtext library*\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#text-classification-with-the-torchtext-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install torchtext\n",
    "\n",
    "Url: https://pypi.org/project/torchtext/\n",
    "```bash\n",
    "conda install -c pytorch torchtext\n",
    "```\n",
    "\n",
    "You may or may not need to manually install the following packages:\n",
    "    \n",
    "```bash\n",
    "pip install chardet\n",
    "pip install -U portalocker>=2.0.0\n",
    "```\n",
    "\n",
    "or with conda\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge 'portalocker>=2.0.0'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.datasets import SST2 # SST2 is the sentiment analysis dataset, binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hide new secretions from the parental units', 0)\n",
      "('contains no wit , only labored gags', 0)\n",
      "('that loves its characters and communicates something rather beautiful about human nature', 1)\n",
      "('remains utterly satisfied to remain the same throughout', 0)\n",
      "('on the worst revenge-of-the-nerds clichés the filmmakers could dredge up', 0)\n",
      "(\"that 's far too tragic to merit such superficial treatment\", 0)\n",
      "('demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .', 1)\n",
      "('of saucy', 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the raw data\n",
    "train_iter = iter(SST2(split='train'))\n",
    "\n",
    "count = 0\n",
    "for item in train_iter:\n",
    "    print(item)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']\n",
      "['contains', 'no', 'wit', ',', 'only', 'labored', 'gags']\n",
      "['that', 'loves', 'its', 'characters', 'and', 'communicates', 'something', 'rather', 'beautiful', 'about', 'human', 'nature']\n",
      "['remains', 'utterly', 'satisfied', 'to', 'remain', 'the', 'same', 'throughout']\n"
     ]
    }
   ],
   "source": [
    "# Check the output of yield_tokens()\n",
    "count = 0\n",
    "for tokens in yield_tokens(iter(SST2(split='train'))): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary\n",
    "\n",
    "First, we build the vocabulary using the `build_vocab_from_iterator` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(iter(SST2(split='train'))), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all strings are converted into integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 16, 1567]\n",
      "[4579, 92, 13266, 38, 1, 7742, 10000]\n",
      "[5, 7100]\n",
      "[224, 10, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check the vocab\n",
    "print(vocab(['here', 'is', 'an', 'example']))\n",
    "print(vocab(['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']))\n",
    "print(vocab(['of', 'saucy']))\n",
    "\n",
    "# What about unknown words, i.e., out-of-vocabulary (OOV) words?\n",
    "print(vocab(['here', 'is', 'a', '@#$@!#$%']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, further define the `text_pipeline` and `label_pipeline` functions, for converting strings to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 1, 16, 1567]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('here is the an example')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Batchify Data \n",
    "\n",
    "Your goal is to define the `Collate_batch` function, which will be used to process the \"raw\" data batch.\n",
    "\n",
    "*Hint*: In the loop of `collate_batch`, the labels, tokens, and the offsets of all the examples are collected into three lists. Finally, the lists are converted into tensors. \n",
    "\n",
    "*Hint*: The `offsets` need to contain the cumulative positions of tokens in the batch. \n",
    "For example, if the batch contains 3 examples, whose lengths are `[1,3,2]`, then the final offsets should be `[0,1,4,6]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        token_ids = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0)) # Note that offsets contains the length (number of tokens) of each example\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # 计算偏移量的累积和，从而得到每个批次数据在合并后的张量中的起始位置\n",
    "    token_ids = torch.cat(token_ids_list) # 得到一个包含所有样本的token IDs的tensor\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the defined `collate_batch` function to create the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "train_iter = SST2(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in this batch:  82\n",
      "Number of examples in one batch:  8\n",
      "Example 0:  tensor([ 4579,    92, 13266,    38,     1,  7742, 10000])\n",
      "Example 7:  tensor([   5, 7100])\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens in this batch: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 0: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 7: ', token_ids[offsets[7]:])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# Number of tokens in this batch:  82\n",
    "# Number of examples in one batch:  8\n",
    "# Example 0:  tensor([ 4579,    92, 13266,    38,     1,  7742, 10000])\n",
    "# Example 7:  tensor([   5, 7100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Define the Model\n",
    "\n",
    "The model consists of two parts of parameters: an embedding layer and a fully connected layer.\n",
    "\n",
    "Your need to first initialize an `nn.EmbeddingBag` instance and a `nn.Linear` instance:\n",
    "- The embedding layer should be initialized with `vocab_size`, `embed_dim`, and `sparse=False`.\n",
    "- The fully connected layer should have `embed_dim` as input size and `num_class` as output size.\n",
    "\n",
    "Then, in the `forward` function, the embedding layer should called with `token_ids` and `offsets` as inputs. The output of embedding layer is fed to the fully connected layer to get the final output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class) # full connection layer\n",
    "        ### END YOUR CODE ###\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        ### START YOUR CODE ###\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        out = out = self.fc(embedded)\n",
    "        ### END YOUR CODE ###\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "train_iter = iter(SST2(split='train'))\n",
    "num_class = len(set([label for (_, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64 # embedding size\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "print('output size:', output.size())\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# output size: torch.Size([8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Define the loss function\n",
    "\n",
    "Cross entropy loss should be used. You can use `torch.nn.CrossEntropyLoss` to define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "### START YOUR CODE ###\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "### END YOUR CODE ###\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the loss function `criterion` works\n",
    "\n",
    "In the cell below, implement the **manual computation** of cross entropy loss. Use the formula $-y_i\\log(\\hat{y_i})$, \n",
    "\n",
    "where $y_i$ is the $i$-th ground truth label in `labels`, and $\\hat{y_i}$ is the predicted probability in `output` of the correponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0, 0, 1, 1]) tensor([ 4579,    92, 13266,    38,     1,  7742, 10000,  2927,    58,   327,\n",
      "            2,    88,  1995,   548,    11,  1791,    18,    54,     4,  6088,\n",
      "           95,   184,   262,    36,   176,   624,   591,   679,  6403,     8,\n",
      "         2010,     1,   287,   701,    25,     1,   252,  5417,   551,     1,\n",
      "          357,   116,  4856,    53,    11,     7,     9,   171,    50,   780,\n",
      "            8,  1840,   120,   952,  1037,  2723,    11,     1,   107,     5,\n",
      "          120,   161,  3473,    14,  7011,  1444,    65,   149,   414,    49,\n",
      "            3,   394,     2,   529,    17,    15,    16,   205,  3149,     6,\n",
      "            5,  7100]) tensor([ 0,  7, 14, 26, 34, 44, 55, 80])\n",
      "batch 0 output: tensor([[  3.3536,  -3.0401],\n",
      "        [  7.7353,  -7.1651],\n",
      "        [ -6.7212,   6.6503],\n",
      "        [  1.4969,  -1.2969],\n",
      "        [ 11.3490, -10.9126],\n",
      "        [  2.9238,  -3.0331],\n",
      "        [ -2.0237,   1.9850],\n",
      "        [ -5.8315,   5.7804]])\n",
      "output shape: torch.Size([8, 2])\n",
      "loss: tensor(0.0102)\n",
      "loss_manual mean: tensor(0.0102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        print(labels,token_ids, offsets)\n",
    "        print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "print('output shape:', output.shape)\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "# Manually calculate the loss\n",
    "loss_manual = []\n",
    "for i in range(output.shape[0]):\n",
    "    ### START YOUR CODE ###\n",
    "    probs = torch.softmax(output[i], dim=0)\n",
    "    l = -torch.log(probs[labels[i]])\n",
    "    ### END YOUR CODE ###\n",
    "    loss_manual.append(l)\n",
    "loss_manual = torch.stack(loss_manual)\n",
    "print('loss_manual mean:', loss_manual.mean())\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# output shape: torch.Size([8, 2])\n",
    "# loss: tensor(0.0115)\n",
    "# loss_manual mean: tensor(0.0115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Train and Evaluate Functions\n",
    "\n",
    "Define train and evaluate functions.\n",
    "\n",
    "You need to implement the forward pass, loss computation, backward propagation, and parameter update in the `train` function. \n",
    "\n",
    "Also, for each batch of data, calculate the total number of correctly predicted examples, by comparing `output` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        ### START YOUR CODE ###\n",
    "        # Forward pass\n",
    "        output = model(token_ids, offsets)\n",
    "        ### END YOUR CODE ###\n",
    "        try:\n",
    "            ### START YOUR CODE ###\n",
    "            # Compute loss\n",
    "            loss = criterion(output, labels)\n",
    "            ### END YOUR CODE ###\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            print('token_ids: ', token_ids)\n",
    "            print('offsets: ', offsets)\n",
    "            raise\n",
    "        ### START YOUR CODE ###\n",
    "        # Backward propagation, grad clipping, and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        # Calculate correct prediction in current batch\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        ### START YOUR CODE ###\n",
    "        # Similar to the code in train function, but without backpropagation\n",
    "        # 前向传播\n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # 计算准确率\n",
    "        total_acc += (output.argmax(1) == label).sum().item()\n",
    "        ### END YOUR CODE ###\n",
    "        total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train, valid, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = SST2(split=\"train\")\n",
    "test_iter = SST2(split=\"test\")\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 7998 batches | accuracy    0.585\n",
      "| epoch   1 |  1000/ 7998 batches | accuracy    0.653\n",
      "| epoch   1 |  1500/ 7998 batches | accuracy    0.718\n",
      "| epoch   1 |  2000/ 7998 batches | accuracy    0.729\n",
      "| epoch   1 |  2500/ 7998 batches | accuracy    0.757\n",
      "| epoch   1 |  3000/ 7998 batches | accuracy    0.786\n",
      "| epoch   1 |  3500/ 7998 batches | accuracy    0.793\n",
      "| epoch   1 |  4000/ 7998 batches | accuracy    0.791\n",
      "| epoch   1 |  4500/ 7998 batches | accuracy    0.807\n",
      "| epoch   1 |  5000/ 7998 batches | accuracy    0.828\n",
      "| epoch   1 |  5500/ 7998 batches | accuracy    0.826\n",
      "| epoch   1 |  6000/ 7998 batches | accuracy    0.828\n",
      "| epoch   1 |  6500/ 7998 batches | accuracy    0.838\n",
      "| epoch   1 |  7000/ 7998 batches | accuracy    0.844\n",
      "| epoch   1 |  7500/ 7998 batches | accuracy    0.841\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 13.53s | valid accuracy    0.836 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  1000/ 7998 batches | accuracy    0.879\n",
      "| epoch   2 |  1500/ 7998 batches | accuracy    0.884\n",
      "| epoch   2 |  2000/ 7998 batches | accuracy    0.876\n",
      "| epoch   2 |  2500/ 7998 batches | accuracy    0.884\n",
      "| epoch   2 |  3000/ 7998 batches | accuracy    0.880\n",
      "| epoch   2 |  3500/ 7998 batches | accuracy    0.881\n",
      "| epoch   2 |  4000/ 7998 batches | accuracy    0.879\n",
      "| epoch   2 |  4500/ 7998 batches | accuracy    0.885\n",
      "| epoch   2 |  5000/ 7998 batches | accuracy    0.877\n",
      "| epoch   2 |  5500/ 7998 batches | accuracy    0.883\n",
      "| epoch   2 |  6000/ 7998 batches | accuracy    0.883\n",
      "| epoch   2 |  6500/ 7998 batches | accuracy    0.881\n",
      "| epoch   2 |  7000/ 7998 batches | accuracy    0.894\n",
      "| epoch   2 |  7500/ 7998 batches | accuracy    0.890\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.17s | valid accuracy    0.873 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 7998 batches | accuracy    0.910\n",
      "| epoch   3 |  1000/ 7998 batches | accuracy    0.909\n",
      "| epoch   3 |  1500/ 7998 batches | accuracy    0.915\n",
      "| epoch   3 |  2000/ 7998 batches | accuracy    0.909\n",
      "| epoch   3 |  2500/ 7998 batches | accuracy    0.901\n",
      "| epoch   3 |  3000/ 7998 batches | accuracy    0.905\n",
      "| epoch   3 |  3500/ 7998 batches | accuracy    0.906\n",
      "| epoch   3 |  4000/ 7998 batches | accuracy    0.906\n",
      "| epoch   3 |  4500/ 7998 batches | accuracy    0.902\n",
      "| epoch   3 |  5000/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  5500/ 7998 batches | accuracy    0.902\n",
      "| epoch   3 |  6000/ 7998 batches | accuracy    0.907\n",
      "| epoch   3 |  6500/ 7998 batches | accuracy    0.889\n",
      "| epoch   3 |  7000/ 7998 batches | accuracy    0.906\n",
      "| epoch   3 |  7500/ 7998 batches | accuracy    0.899\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.58s | valid accuracy    0.886 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 7998 batches | accuracy    0.917\n",
      "| epoch   4 |  1000/ 7998 batches | accuracy    0.921\n",
      "| epoch   4 |  1500/ 7998 batches | accuracy    0.918\n",
      "| epoch   4 |  2000/ 7998 batches | accuracy    0.920\n",
      "| epoch   4 |  2500/ 7998 batches | accuracy    0.917\n",
      "| epoch   4 |  3000/ 7998 batches | accuracy    0.911\n",
      "| epoch   4 |  3500/ 7998 batches | accuracy    0.917\n",
      "| epoch   4 |  4000/ 7998 batches | accuracy    0.917\n",
      "| epoch   4 |  4500/ 7998 batches | accuracy    0.918\n",
      "| epoch   4 |  5000/ 7998 batches | accuracy    0.912\n",
      "| epoch   4 |  5500/ 7998 batches | accuracy    0.912\n",
      "| epoch   4 |  6000/ 7998 batches | accuracy    0.912\n",
      "| epoch   4 |  6500/ 7998 batches | accuracy    0.910\n",
      "| epoch   4 |  7000/ 7998 batches | accuracy    0.914\n",
      "| epoch   4 |  7500/ 7998 batches | accuracy    0.917\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.52s | valid accuracy    0.885 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 7998 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 7998 batches | accuracy    0.943\n",
      "| epoch   5 |  1500/ 7998 batches | accuracy    0.941\n",
      "| epoch   5 |  2000/ 7998 batches | accuracy    0.941\n",
      "| epoch   5 |  2500/ 7998 batches | accuracy    0.939\n",
      "| epoch   5 |  3000/ 7998 batches | accuracy    0.939\n",
      "| epoch   5 |  3500/ 7998 batches | accuracy    0.940\n",
      "| epoch   5 |  4000/ 7998 batches | accuracy    0.937\n",
      "| epoch   5 |  4500/ 7998 batches | accuracy    0.942\n",
      "| epoch   5 |  5000/ 7998 batches | accuracy    0.944\n",
      "| epoch   5 |  5500/ 7998 batches | accuracy    0.942\n",
      "| epoch   5 |  6000/ 7998 batches | accuracy    0.934\n",
      "| epoch   5 |  6500/ 7998 batches | accuracy    0.942\n",
      "| epoch   5 |  7000/ 7998 batches | accuracy    0.943\n",
      "| epoch   5 |  7500/ 7998 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.66s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 7998 batches | accuracy    0.943\n",
      "| epoch   6 |  1000/ 7998 batches | accuracy    0.949\n",
      "| epoch   6 |  1500/ 7998 batches | accuracy    0.944\n",
      "| epoch   6 |  2000/ 7998 batches | accuracy    0.940\n",
      "| epoch   6 |  2500/ 7998 batches | accuracy    0.944\n",
      "| epoch   6 |  3000/ 7998 batches | accuracy    0.945\n",
      "| epoch   6 |  3500/ 7998 batches | accuracy    0.943\n",
      "| epoch   6 |  4000/ 7998 batches | accuracy    0.949\n",
      "| epoch   6 |  4500/ 7998 batches | accuracy    0.939\n",
      "| epoch   6 |  5000/ 7998 batches | accuracy    0.941\n",
      "| epoch   6 |  5500/ 7998 batches | accuracy    0.948\n",
      "| epoch   6 |  6000/ 7998 batches | accuracy    0.947\n",
      "| epoch   6 |  6500/ 7998 batches | accuracy    0.942\n",
      "| epoch   6 |  7000/ 7998 batches | accuracy    0.948\n",
      "| epoch   6 |  7500/ 7998 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 13.10s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 7998 batches | accuracy    0.943\n",
      "| epoch   7 |  1000/ 7998 batches | accuracy    0.946\n",
      "| epoch   7 |  1500/ 7998 batches | accuracy    0.953\n",
      "| epoch   7 |  2000/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  2500/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  3000/ 7998 batches | accuracy    0.945\n",
      "| epoch   7 |  3500/ 7998 batches | accuracy    0.949\n",
      "| epoch   7 |  4000/ 7998 batches | accuracy    0.947\n",
      "| epoch   7 |  4500/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  5000/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  5500/ 7998 batches | accuracy    0.946\n",
      "| epoch   7 |  6000/ 7998 batches | accuracy    0.949\n",
      "| epoch   7 |  6500/ 7998 batches | accuracy    0.944\n",
      "| epoch   7 |  7000/ 7998 batches | accuracy    0.945\n",
      "| epoch   7 |  7500/ 7998 batches | accuracy    0.948\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.85s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  1000/ 7998 batches | accuracy    0.953\n",
      "| epoch   8 |  1500/ 7998 batches | accuracy    0.950\n",
      "| epoch   8 |  2000/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  2500/ 7998 batches | accuracy    0.948\n",
      "| epoch   8 |  3000/ 7998 batches | accuracy    0.945\n",
      "| epoch   8 |  3500/ 7998 batches | accuracy    0.953\n",
      "| epoch   8 |  4000/ 7998 batches | accuracy    0.955\n",
      "| epoch   8 |  4500/ 7998 batches | accuracy    0.956\n",
      "| epoch   8 |  5000/ 7998 batches | accuracy    0.950\n",
      "| epoch   8 |  5500/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  6000/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  6500/ 7998 batches | accuracy    0.946\n",
      "| epoch   8 |  7000/ 7998 batches | accuracy    0.945\n",
      "| epoch   8 |  7500/ 7998 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.68s | valid accuracy    0.903 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  1000/ 7998 batches | accuracy    0.949\n",
      "| epoch   9 |  1500/ 7998 batches | accuracy    0.955\n",
      "| epoch   9 |  2000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  2500/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  3000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  3500/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  4000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  4500/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  5000/ 7998 batches | accuracy    0.943\n",
      "| epoch   9 |  5500/ 7998 batches | accuracy    0.944\n",
      "| epoch   9 |  6000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  6500/ 7998 batches | accuracy    0.951\n",
      "| epoch   9 |  7000/ 7998 batches | accuracy    0.959\n",
      "| epoch   9 |  7500/ 7998 batches | accuracy    0.951\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 13.01s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  1000/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  1500/ 7998 batches | accuracy    0.948\n",
      "| epoch  10 |  2000/ 7998 batches | accuracy    0.954\n",
      "| epoch  10 |  2500/ 7998 batches | accuracy    0.945\n",
      "| epoch  10 |  3000/ 7998 batches | accuracy    0.949\n",
      "| epoch  10 |  3500/ 7998 batches | accuracy    0.947\n",
      "| epoch  10 |  4000/ 7998 batches | accuracy    0.954\n",
      "| epoch  10 |  4500/ 7998 batches | accuracy    0.949\n",
      "| epoch  10 |  5000/ 7998 batches | accuracy    0.951\n",
      "| epoch  10 |  5500/ 7998 batches | accuracy    0.946\n",
      "| epoch  10 |  6000/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  6500/ 7998 batches | accuracy    0.952\n",
      "| epoch  10 |  7000/ 7998 batches | accuracy    0.949\n",
      "| epoch  10 |  7500/ 7998 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.10s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Test Data\n",
    "\n",
    "This is a necessary step. But since the `test` split of SST2 is not annotated, we will use the `dev` split here to pretend it is the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.902\n"
     ]
    }
   ],
   "source": [
    "accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "\n",
    "# Your test accuracy should be around 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Test the model with a few unannotated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a positive sentiment.\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = ['negative', 'positive']\n",
    "\n",
    "def predict(text, model, vocab, tokenizer, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(vocab(tokenizer(text)), device=device)\n",
    "        output = model(text, torch.tensor([0], device=device))\n",
    "        return labels[output.argmax(1).item()]\n",
    "\n",
    "ex_text_str = \"A very well-made, funny and entertaining picture.\"\n",
    "print(\"This is a %s sentiment.\" % (predict(ex_text_str, model, vocab, tokenizer, sentiment_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully built and trained a neural network model to classify sentiment in text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
