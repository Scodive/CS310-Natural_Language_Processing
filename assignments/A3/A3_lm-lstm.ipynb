{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 2). Named Entity Recognition with Bi-LSTM\n",
    "\n",
    "**Total points**: 30 + 20 bonus points\n",
    "\n",
    "In this assignment, you will train a bidirectional LSTM model on the CoNLL2003 English named entity recognition task set and evaluate its performance.\n",
    "\n",
    "For the bonus questions, submit them as separate notebook files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0. Import Necessary Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            \n",
    "        # 词嵌入\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # LSTM前向传播\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # output shape: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 应用dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # 全连接层\n",
    "        output = self.fc(output)\n",
    "        # output shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                weight.new_zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
    "    train_losses = []\n",
    "    test_perplexities = []\n",
    "    best_perplexity = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(inputs)\n",
    "            \n",
    "            # 重塑输出和目标以计算损失\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "            \n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # 评估模型\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_words = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output, hidden = model(inputs)\n",
    "                output = output.view(-1, output.size(-1))\n",
    "                targets = targets.view(-1)\n",
    "                \n",
    "                loss = criterion(output, targets)\n",
    "                total_loss += loss.item() * targets.size(0)\n",
    "                total_words += targets.size(0)\n",
    "        \n",
    "        perplexity = np.exp(total_loss / total_words)\n",
    "        test_perplexities.append(perplexity)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Test Perplexity: {perplexity:.4f}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "    \n",
    "    return train_losses, test_perplexities\n",
    "\n",
    "def generate_text(model, vocab, prefix, max_length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = prefix.lower().split()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 转换前缀词为索引\n",
    "    input_indices = [vocab.word2idx.get(word, vocab.word2idx['<UNK>']) for word in words]\n",
    "    input_tensor = torch.LongTensor([input_indices]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        generated_words = words.copy()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            \n",
    "            # 获取最后一个时间步的预测\n",
    "            word_weights = output[0, -1].div(temperature).exp()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            \n",
    "            # 将生成的词添加到结果中\n",
    "            generated_word = vocab.idx2word[word_idx.item()]\n",
    "            generated_words.append(generated_word)\n",
    "            \n",
    "            # 准备下一个输入\n",
    "            input_tensor = torch.LongTensor([[word_idx]]).to(device)\n",
    "            \n",
    "            if generated_word == '<EOS>':\n",
    "                break\n",
    "    \n",
    "    return ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "EMBEDDING_DIM = 300  # 增加嵌入维度\n",
    "HIDDEN_DIM = 512    # 增加隐藏层维度\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "SEQUENCE_LENGTH = 35\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "file_path = 'Harry_Potter_all_books_preprocessed.txt'\n",
    "train_dataset, test_dataset, vocab = load_and_preprocess_data(file_path, SEQUENCE_LENGTH)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTMModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 训练模型\n",
    "train_losses, test_perplexities = train_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, \n",
    "    NUM_EPOCHS, DEVICE\n",
    ")\n",
    "\n",
    "# 生成示例文本\n",
    "test_prefixes = [\n",
    "    \"Harry looked at\",\n",
    "    \"The castle was\",\n",
    "    \"Hermione said\",\n",
    "    \"Ron couldn't\",\n",
    "    \"Dumbledore smiled\"\n",
    "]\n",
    "\n",
    "print(\"\\n生成的文本样例:\")\n",
    "for prefix in test_prefixes:\n",
    "    generated = generate_text(model, vocab, prefix)\n",
    "    print(f\"\\n前缀: {prefix}\")\n",
    "    print(f\"生成: {generated}\")\n",
    "\n",
    "# 绘制训练损失和测试困惑度曲线\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('训练损失')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_perplexities)\n",
    "plt.title('测试困惑度')\n",
    "plt.xlabel('轮次')\n",
    "plt.ylabel('困惑度')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
