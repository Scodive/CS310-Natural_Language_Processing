{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3. Recurrent Neural Networks for Language Modeling \n",
    "\n",
    "**Total points**: \n",
    "\n",
    "In this assignment, you will train a vanilla RNN-based language model on the Harry Potter text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sco/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/sco/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/sco/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# 下载必要的NLTK数据\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Processing\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<BOS>': 2, '<EOS>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<BOS>', 3: '<EOS>'}\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "    def build_vocab(self, text):\n",
    "        # 统计词频\n",
    "        word_counts = Counter(text)\n",
    "        # 只保留频率大于min_freq的词\n",
    "        valid_words = [word for word, count in word_counts.items() if count >= self.min_freq]\n",
    "        \n",
    "        # 构建词典\n",
    "        for word in valid_words:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "                self.idx2word[len(self.idx2word)] = word\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, vocab, seq_length):\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # 将文本转换为索引\n",
    "        self.data = [self.vocab.word2idx.get(word, self.vocab.word2idx['<UNK>']) \n",
    "                    for word in text]\n",
    "        \n",
    "        # 创建序列\n",
    "        self.sequences = []\n",
    "        for i in range(0, len(self.data) - seq_length, 1):\n",
    "            seq = self.data[i:i + seq_length]\n",
    "            target = self.data[i + 1:i + seq_length + 1]\n",
    "            self.sequences.append((seq, target))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq, target = self.sequences[idx]\n",
    "        return torch.LongTensor(seq), torch.LongTensor(target)\n",
    "\n",
    "# 加载和预处理数据\n",
    "def load_and_preprocess_data(file_path, seq_length=35):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    \n",
    "    # 使用简单的空格分词而不是NLTK\n",
    "    words = text.split()\n",
    "    # 或者使用更细致的分词规则\n",
    "    # words = [word for word in text.replace('\\n', ' ').replace('\\t', ' ').split(' ') if word.strip()]\n",
    "    \n",
    "    # 构建词汇表\n",
    "    vocab = Vocabulary(min_freq=2)\n",
    "    vocab.build_vocab(words)\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    split_idx = int(len(words) * 0.9)\n",
    "    train_words = words[:split_idx]\n",
    "    test_words = words[split_idx:]\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = TextDataset(train_words, vocab, seq_length)\n",
    "    test_dataset = TextDataset(test_words, vocab, seq_length)\n",
    "    \n",
    "    return train_dataset, test_dataset, vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Model Implementation\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            \n",
    "        # 词嵌入\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # RNN前向传播\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output shape: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 应用dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # 全连接层\n",
    "        output = self.fc(output)\n",
    "        # output shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.num_layers, batch_size, self.hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1912], Loss: 7.3696\n",
      "Epoch [1/5], Step [200/1912], Loss: 6.7744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m train_losses, test_perplexities \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m    112\u001b[0m     model, train_loader, test_loader, criterion, optimizer, \n\u001b[1;32m    113\u001b[0m     NUM_EPOCHS, DEVICE\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# 生成示例文本\u001b[39;00m\n\u001b[1;32m    117\u001b[0m test_prefixes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHarry looked at\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe castle was\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDumbledore smiled\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs, device, print_every)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# 梯度裁剪\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 3. Training and Evaluation Functions\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, \n",
    "                num_epochs, device, print_every=100):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    test_perplexities = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        hidden = None\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            hidden = hidden.detach()  # 分离计算图\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # 梯度裁剪\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % print_every == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {total_loss/print_every:.4f}')\n",
    "                total_loss = 0\n",
    "        \n",
    "        # 计算测试集困惑度\n",
    "        test_perplexity = evaluate(model, test_loader, criterion, device)\n",
    "        test_perplexities.append(test_perplexity)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test Perplexity: {test_perplexity:.4f}')\n",
    "        \n",
    "    return train_losses, test_perplexities\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "def generate_text(model, vocab, prefix, max_length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = prefix.lower().split()\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            # 将当前序列转换为tensor\n",
    "            curr_seq = torch.LongTensor([[vocab.word2idx.get(w, vocab.word2idx['<UNK>']) \n",
    "                                        for w in words[-1:]]])\n",
    "            \n",
    "            # 获取预测\n",
    "            output, hidden = model(curr_seq, hidden)\n",
    "            \n",
    "            # 应用temperature\n",
    "            output_dist = output.div(temperature).exp()\n",
    "            top_idx = torch.multinomial(output_dist.view(-1), 1)[0]\n",
    "            \n",
    "            # 添加预测的词\n",
    "            predicted_word = vocab.idx2word[top_idx.item()]\n",
    "            words.append(predicted_word)\n",
    "            \n",
    "            if predicted_word == '<EOS>':\n",
    "                break\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "### 4. Main Training Loop\n",
    "\n",
    "# 设置参数\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "SEQUENCE_LENGTH = 35\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "file_path = '/Users/sco/Desktop/CS310-Natural_Language_Processing/assignments/A3/Harry_Potter_all_books_preprocessed.txt'\n",
    "train_dataset, test_dataset, vocab = load_and_preprocess_data(file_path, SEQUENCE_LENGTH)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 初始化模型\n",
    "model = RNNModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 训练模型\n",
    "train_losses, test_perplexities = train_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, \n",
    "    NUM_EPOCHS, DEVICE\n",
    ")\n",
    "\n",
    "# 生成示例文本\n",
    "test_prefixes = [\n",
    "    \"Harry looked at\",\n",
    "    \"The castle was\",\n",
    "    \"Hermione said\",\n",
    "    \"Ron couldn't\",\n",
    "    \"Dumbledore smiled\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerated Text Samples:\")\n",
    "for prefix in test_prefixes:\n",
    "    generated = generate_text(model, vocab, prefix)\n",
    "    print(f\"\\nPrefix: {prefix}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "\n",
    "# 绘制训练损失和测试困惑度曲线\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_perplexities)\n",
    "plt.title('Test Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
