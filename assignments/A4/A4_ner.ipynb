{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Long Short Term Memory (LSTM) Network for Named Entity Recognition (NER)\n",
    "\n",
    "**Total points**: 50 + (10 bonus)\n",
    "\n",
    "In this assignment, you will implement a Long Short Term Memory (LSTM) network for Named Entity Recognition (NER). \n",
    "\n",
    "Re-use the code in Lab 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载GloVe词向量...\n",
      "GloVe词向量加载完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path, word2idx, label2idx):\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.word2idx = word2idx\n",
    "        self.label2idx = label2idx\n",
    "        \n",
    "        current_sentence = []\n",
    "        current_labels = []\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    if current_sentence:\n",
    "                        self.sentences.append([word2idx.get(w.lower(), word2idx['<UNK>']) for w in current_sentence])\n",
    "                        self.labels.append([label2idx[l] for l in current_labels])\n",
    "                        current_sentence = []\n",
    "                        current_labels = []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    current_sentence.append(parts[0])\n",
    "                    current_labels.append(parts[-1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "def build_vocab(train_path):\n",
    "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    label2idx = {}\n",
    "    word_freq = defaultdict(int)\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                word = parts[0].lower()\n",
    "                label = parts[-1]\n",
    "                word_freq[word] += 1\n",
    "                if label not in label2idx:\n",
    "                    label2idx[label] = len(label2idx)\n",
    "    \n",
    "    for word, freq in word_freq.items():\n",
    "        if freq > 1 and word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "    \n",
    "    return word2idx, label2idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sentences, labels = zip(*batch)\n",
    "    max_len = len(sentences[0])\n",
    "    \n",
    "    padded_sentences = []\n",
    "    padded_labels = []\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        padding_len = max_len - len(sentence)\n",
    "        padded_sentence = torch.cat([sentence, torch.zeros(padding_len, dtype=torch.long)])\n",
    "        padded_label = torch.cat([label, torch.zeros(padding_len, dtype=torch.long)])\n",
    "        padded_sentences.append(padded_sentence)\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    return torch.stack(padded_sentences), torch.stack(padded_labels)\n",
    "\n",
    "def load_glove_embeddings(word2idx, embed_dim=100):\n",
    "    glove_path = 'glove.6B.100d.txt'\n",
    "    print(\"正在加载GloVe词向量...\")\n",
    "    \n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), embed_dim))\n",
    "    \n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                if word in word2idx:\n",
    "                    vector = np.array(values[1:], dtype='float32')\n",
    "                    embeddings[word2idx[word]] = vector\n",
    "        print(\"GloVe词向量加载完成！\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载词向量时出错: {e}\")\n",
    "        print(\"使用随机初始化的词向量继续...\")\n",
    "    \n",
    "    return torch.FloatTensor(embeddings)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义的collate_fn函数，用于处理不同长度的序列\n",
    "    \"\"\"\n",
    "    # 将batch中的样本按句子长度排序（降序）\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    # 分离句子和标签\n",
    "    sentences, labels = zip(*batch)\n",
    "    \n",
    "    # 获取这个batch中最长句子的长度\n",
    "    max_len = len(sentences[0])\n",
    "    \n",
    "    # 对句子和标签进行padding\n",
    "    padded_sentences = []\n",
    "    padded_labels = []\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        # 计算需要padding的长度\n",
    "        padding_len = max_len - len(sentence)\n",
    "        \n",
    "        # 对句子进行padding（使用0，即<PAD>的索引）\n",
    "        padded_sentence = torch.cat([sentence, torch.zeros(padding_len, dtype=torch.long)])\n",
    "        \n",
    "        # 对标签进行padding（使用0，即<PAD>的索引）\n",
    "        padded_label = torch.cat([label, torch.zeros(padding_len, dtype=torch.long)])\n",
    "        \n",
    "        padded_sentences.append(padded_sentence)\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    # 将列表转换为tensor\n",
    "    padded_sentences = torch.stack(padded_sentences)\n",
    "    padded_labels = torch.stack(padded_labels)\n",
    "    \n",
    "    return padded_sentences, padded_labels\n",
    "\n",
    "# 然后修改DataLoader的创建\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "def build_vocab(train_path):\n",
    "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    label2idx = {}\n",
    "    word_freq = defaultdict(int)\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                word = parts[0].lower()\n",
    "                label = parts[-1]\n",
    "                word_freq[word] += 1\n",
    "                if label not in label2idx:\n",
    "                    label2idx[label] = len(label2idx)\n",
    "    \n",
    "    for word, freq in word_freq.items():\n",
    "        if freq > 1 and word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "    \n",
    "    return word2idx, label2idx\n",
    "\n",
    "# 初始化数据\n",
    "train_path = 'data/train.txt'\n",
    "dev_path = 'data/dev.txt'\n",
    "test_path = 'data/test.txt'\n",
    "\n",
    "word2idx, label2idx = build_vocab(train_path)\n",
    "train_dataset = CoNLLDataset(train_path, word2idx, label2idx)\n",
    "dev_dataset = CoNLLDataset(dev_path, word2idx, label2idx)\n",
    "test_dataset = CoNLLDataset(test_path, word2idx, label2idx)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# 加载预训练词嵌入\n",
    "pretrained_embeddings = load_glove_embeddings(word2idx)\n",
    "\n",
    "# 在加载完预训练词向量之后，添加以下代码：\n",
    "\n",
    "class BiLSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_tags, dropout=0.5, pretrained_embeddings=None):\n",
    "        super(BiLSTM_NER, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 词嵌入层\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "            # 冻结预训练的词向量\n",
    "            self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        # 增加embedding dropout\n",
    "        self.embed_dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # BiLSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim // 2,  # 因为是双向的，所以hidden_dim要除以2\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # 增加层归一化\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 线性层\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_tags)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeds = self.word_embeddings(x)\n",
    "        embeds = self.embed_dropout(embeds)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        return tag_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "Epoch 1/5\n",
      "Train Loss: 1.3262, Train F1: 0.7881\n",
      "Dev Loss: 0.9756, Dev F1: 0.5086\n",
      "Learning rate: 0.001000\n",
      "保存模型到 best_model.pt (F1: 0.5086)\n",
      "Epoch 2/5\n",
      "Train Loss: 0.9678, Train F1: 0.7540\n",
      "Dev Loss: 0.8506, Dev F1: 0.7202\n",
      "Learning rate: 0.001000\n",
      "保存模型到 best_model.pt (F1: 0.7202)\n",
      "Epoch 3/5\n",
      "Train Loss: 0.8137, Train F1: 0.5907\n",
      "Dev Loss: 0.7648, Dev F1: 0.5754\n",
      "Learning rate: 0.001000\n",
      "验证集F1未改善，已经1个epoch\n",
      "Epoch 4/5\n",
      "Train Loss: 0.6934, Train F1: 0.4730\n",
      "Dev Loss: 0.6937, Dev F1: 0.6210\n",
      "Learning rate: 0.001000\n",
      "验证集F1未改善，已经2个epoch\n",
      "Epoch 5/5\n",
      "Train Loss: 0.6074, Train F1: 0.7235\n",
      "Dev Loss: 0.6348, Dev F1: 0.6833\n",
      "Learning rate: 0.000500\n",
      "验证集F1未改善，已经3个epoch\n",
      "成功加载最佳模型 (Epoch 2, Best F1: 0.7202)\n",
      "\n",
      "Final Test F1: 0.9412\n"
     ]
    }
   ],
   "source": [
    "from utils import get_tag_indices_from_scores\n",
    "from metrics import MetricsHandler\n",
    "\n",
    "labels_str = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "labels_int = list(range(len(labels_str)))\n",
    "train_metrics = MetricsHandler(labels_int)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_func, train_metrics, num_epochs=5, **kwargs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算损失\n",
    "            batch_size, seq_len, num_tags = outputs.size()\n",
    "            outputs_reshaped = outputs.view(-1, num_tags)\n",
    "            targets_reshaped = targets.view(-1)\n",
    "            \n",
    "            loss = loss_func(outputs_reshaped, targets_reshaped)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 计算预测结果\n",
    "            predictions = get_tag_indices_from_scores(outputs.detach().cpu().numpy())\n",
    "            predictions = predictions.reshape(-1)\n",
    "            true_values = targets.cpu().numpy().reshape(-1)\n",
    "            \n",
    "            # 过滤掉padding位置\n",
    "            mask = (true_values != 0)  # 0是PAD的索引\n",
    "            predictions = predictions[mask]\n",
    "            true_values = true_values[mask]\n",
    "            \n",
    "            # 更新指标\n",
    "            train_metrics.update(predictions, true_values)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(data_loader.dataset)\n",
    "        losses.append(epoch_loss)\n",
    "    \n",
    "    return model, train_metrics, losses\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, metrics):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算损失\n",
    "            batch_size, seq_len, num_tags = outputs.size()\n",
    "            outputs_reshaped = outputs.view(-1, num_tags)\n",
    "            targets_reshaped = targets.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # 修改预测结果的获取方式\n",
    "            predictions = torch.argmax(outputs, dim=2)  # [batch_size, seq_len]\n",
    "            predictions = predictions.cpu().numpy().reshape(-1)\n",
    "            true_values = targets.cpu().numpy().reshape(-1)\n",
    "            \n",
    "            # 过滤掉padding位置\n",
    "            mask = (true_values != 0)  # 0是PAD的索引\n",
    "            predictions = predictions[mask]\n",
    "            true_values = true_values[mask]\n",
    "            \n",
    "            # 在更新指标之前收集当前批次的指标\n",
    "            metrics.collect()\n",
    "            # 更新指标\n",
    "            metrics.update(predictions, true_values)\n",
    "    \n",
    "    return total_loss / len(data_loader.dataset), metrics\n",
    "\n",
    "# 初始化模型参数\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 100  # GloVe维度\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "num_tags = len(label2idx)\n",
    "dropout = 0.3  # 降低dropout率\n",
    "\n",
    "# 创建模型实例\n",
    "model = BiLSTM_NER(vocab_size, embedding_dim, hidden_dim, num_layers, num_tags, \n",
    "                   dropout=dropout, pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略PAD的损失\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)  # 降低学习率和权重衰减\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                factor=0.5, patience=2, \n",
    "                                                verbose=True, min_lr=1e-6)\n",
    "\n",
    "# 如果有GPU可用，将模型移到GPU\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# 训练循环\n",
    "print(\"开始训练...\")\n",
    "num_epochs = 5\n",
    "best_f1 = -1\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_metrics = MetricsHandler(labels_int)\n",
    "    dev_metrics = MetricsHandler(labels_int)\n",
    "    \n",
    "    # 训练阶段\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, targets = batch\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        batch_size, seq_len, num_tags = outputs.size()\n",
    "        outputs_reshaped = outputs.view(-1, num_tags)\n",
    "        targets_reshaped = targets.view(-1)\n",
    "        \n",
    "        # 检查数值是否有效\n",
    "        if torch.isnan(outputs_reshaped).any():\n",
    "            print(f\"Warning: NaN in outputs at batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "        \n",
    "        # 检查损失是否有效\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Warning: NaN loss at batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        # 更严格的梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        \n",
    "        # 检查梯度是否有效\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"Warning: NaN gradient in {name}\")\n",
    "                    continue\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss = loss.item()\n",
    "        if not np.isnan(current_loss):\n",
    "            total_loss += current_loss * inputs.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            predictions = predictions.detach().cpu().numpy().reshape(-1)\n",
    "            true_values = targets.cpu().numpy().reshape(-1)\n",
    "            \n",
    "            mask = (true_values != 0)\n",
    "            predictions = predictions[mask]\n",
    "            true_values = true_values[mask]\n",
    "            \n",
    "            train_metrics.collect()\n",
    "            train_metrics.update(predictions, true_values)\n",
    "    \n",
    "    # 计算训练集性能\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_metrics_dict = train_metrics.get_metrics()\n",
    "    train_f1 = train_metrics_dict['F1-score'][-1] if train_metrics_dict['F1-score'] else 0.0\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    dev_total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            inputs, targets = batch\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            batch_size, seq_len, num_tags = outputs.size()\n",
    "            outputs_reshaped = outputs.view(-1, num_tags)\n",
    "            targets_reshaped = targets.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "            if not torch.isnan(loss):\n",
    "                dev_total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            predictions = predictions.cpu().numpy().reshape(-1)\n",
    "            true_values = targets.cpu().numpy().reshape(-1)\n",
    "            \n",
    "            mask = (true_values != 0)\n",
    "            predictions = predictions[mask]\n",
    "            true_values = true_values[mask]\n",
    "            \n",
    "            dev_metrics.collect()\n",
    "            dev_metrics.update(predictions, true_values)\n",
    "    \n",
    "    dev_loss = dev_total_loss / len(dev_loader.dataset)\n",
    "    dev_metrics_dict = dev_metrics.get_metrics()\n",
    "    dev_f1 = dev_metrics_dict['F1-score'][-1] if dev_metrics_dict['F1-score'] else 0.0\n",
    "    \n",
    "    # 学习率调整\n",
    "    scheduler.step(dev_f1)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}')\n",
    "    print(f'Dev Loss: {dev_loss:.4f}, Dev F1: {dev_f1:.4f}')\n",
    "    print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if dev_f1 > best_f1:\n",
    "        best_f1 = dev_f1\n",
    "        print(f\"保存模型到 best_model.pt (F1: {dev_f1:.4f})\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_f1': best_f1,\n",
    "        }, 'best_model.pt')\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"验证集F1未改善，已经{no_improve}个epoch\")\n",
    "    \n",
    "    # 早停检查\n",
    "    if no_improve >= patience:\n",
    "        print(f\"早停：验证集F1在{patience}个epoch内未改善\")\n",
    "        break\n",
    "\n",
    "# 加载最佳模型进行测试\n",
    "try:\n",
    "    checkpoint = torch.load('best_model.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"成功加载最佳模型 (Epoch {checkpoint['epoch']+1}, Best F1: {checkpoint['best_f1']:.4f})\")\n",
    "except Exception as e:\n",
    "    print(f\"加载模型时出错: {e}\")\n",
    "    print(\"使用当前模型进行测试...\")\n",
    "\n",
    "# 最终测试\n",
    "model.eval()\n",
    "test_metrics = MetricsHandler(labels_int)\n",
    "with torch.no_grad():\n",
    "    test_loss, test_metrics = evaluate_model(model, test_loader, criterion, test_metrics)\n",
    "    test_metrics_dict = test_metrics.get_metrics()\n",
    "    test_f1 = test_metrics_dict['F1-score'][-1] if test_metrics_dict['F1-score'] else 0.0\n",
    "\n",
    "print(f'\\nFinal Test F1: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载GloVe词向量...\n",
      "GloVe词向量加载完成！\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sco/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMM Batch 10/59, Loss: 0.1508\n",
      "MEMM Batch 20/59, Loss: 0.0380\n",
      "MEMM Batch 30/59, Loss: 0.0131\n",
      "MEMM Batch 40/59, Loss: 0.0064\n",
      "MEMM Batch 50/59, Loss: 0.0076\n",
      "MEMM - Train Loss: 0.1618, Dev F1: 0.0512\n",
      "CRF Batch 10/59, Loss: 11094.7148\n",
      "CRF Batch 20/59, Loss: 11678.8809\n",
      "CRF Batch 30/59, Loss: 11521.1592\n",
      "CRF Batch 40/59, Loss: 13120.8760\n",
      "CRF Batch 50/59, Loss: 11791.7402\n",
      "CRF - Train Loss: 11576.5412, Dev F1: 0.1704\n",
      "MEMM LR: 0.001000\n",
      "CRF LR: 0.000100\n",
      "\n",
      "Epoch 2/20\n",
      "MEMM Batch 10/59, Loss: 0.0079\n",
      "MEMM Batch 20/59, Loss: 0.0047\n",
      "MEMM Batch 30/59, Loss: 0.0039\n",
      "MEMM Batch 40/59, Loss: 0.0022\n",
      "MEMM Batch 50/59, Loss: 0.0058\n",
      "MEMM - Train Loss: 0.0044, Dev F1: 0.0741\n",
      "CRF Batch 10/59, Loss: 11476.9453\n",
      "CRF Batch 20/59, Loss: 12647.1357\n",
      "CRF Batch 30/59, Loss: 12099.1865\n",
      "CRF Batch 40/59, Loss: 11826.0596\n",
      "CRF Batch 50/59, Loss: 11513.5566\n",
      "CRF - Train Loss: 11590.3948, Dev F1: 0.0730\n",
      "MEMM LR: 0.001000\n",
      "CRF LR: 0.000100\n",
      "\n",
      "Epoch 3/20\n",
      "MEMM Batch 10/59, Loss: 0.0048\n",
      "MEMM Batch 20/59, Loss: 0.0013\n",
      "MEMM Batch 30/59, Loss: 0.0014\n",
      "MEMM Batch 40/59, Loss: 0.0066\n",
      "MEMM Batch 50/59, Loss: 0.0033\n",
      "MEMM - Train Loss: 0.0019, Dev F1: 0.1213\n",
      "CRF Batch 10/59, Loss: 11277.9932\n",
      "CRF Batch 20/59, Loss: 11003.6875\n",
      "CRF Batch 30/59, Loss: 10574.0977\n",
      "CRF Batch 40/59, Loss: 10573.3896\n",
      "CRF Batch 50/59, Loss: 12133.6523\n",
      "CRF - Train Loss: 11572.4229, Dev F1: 0.1510\n",
      "MEMM LR: 0.001000\n",
      "CRF LR: 0.000100\n",
      "\n",
      "Epoch 4/20\n",
      "MEMM Batch 10/59, Loss: 0.0008\n",
      "MEMM Batch 20/59, Loss: 0.0008\n",
      "MEMM Batch 30/59, Loss: 0.0004\n",
      "MEMM Batch 40/59, Loss: 0.0010\n",
      "MEMM Batch 50/59, Loss: 0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 646\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRF Test F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1_crf\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# 运行训练和评估\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m train_and_evaluate_models()\n",
      "Cell \u001b[0;32mIn[46], line 549\u001b[0m, in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# 评估MEMM模型\u001b[39;00m\n\u001b[1;32m    548\u001b[0m memm_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 549\u001b[0m dev_f1_memm \u001b[38;5;241m=\u001b[39m evaluate_with_beam_search(memm_model, dev_loader)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMEMM - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemm_train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dev F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdev_f1_memm\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# 训练CRF模型\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 381\u001b[0m, in \u001b[0;36mevaluate_with_beam_search\u001b[0;34m(model, data_loader, beam_size)\u001b[0m\n\u001b[1;32m    378\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    379\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 381\u001b[0m predictions \u001b[38;5;241m=\u001b[39m beam_search_decode(model, inputs, beam_size)\n\u001b[1;32m    383\u001b[0m mask \u001b[38;5;241m=\u001b[39m (targets \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    384\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions[mask]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[46], line 351\u001b[0m, in \u001b[0;36mbeam_search_decode\u001b[0;34m(model, sentence, beam_size)\u001b[0m\n\u001b[1;32m    348\u001b[0m     prev_tags[\u001b[38;5;241m0\u001b[39m, i] \u001b[38;5;241m=\u001b[39m tag\n\u001b[1;32m    350\u001b[0m current_input \u001b[38;5;241m=\u001b[39m sentence[b:b\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 351\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(current_input, prev_tags)\n\u001b[1;32m    352\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(outputs[\u001b[38;5;241m0\u001b[39m, t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    354\u001b[0m topk_probs, topk_tags \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39mtopk(beam_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 324\u001b[0m, in \u001b[0;36mMEMM_NER.forward\u001b[0;34m(self, x, prev_tags)\u001b[0m\n\u001b[1;32m    322\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(lstm_out)\n\u001b[1;32m    323\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(lstm_out)\n\u001b[0;32m--> 324\u001b[0m tag_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden2tag(lstm_out)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tag_space\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# 定义MetricsHandler类\n",
    "class MetricsHandler:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self.predictions = []\n",
    "        self.true_values = []\n",
    "        self.metrics = {\n",
    "            'Precision': [],\n",
    "            'Recall': [],\n",
    "            'F1-score': []\n",
    "        }\n",
    "    \n",
    "    def collect(self):\n",
    "        if self.predictions and self.true_values:\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                self.true_values, \n",
    "                self.predictions, \n",
    "                labels=self.labels, \n",
    "                average='weighted',\n",
    "                zero_division=0\n",
    "            )\n",
    "            self.metrics['Precision'].append(precision)\n",
    "            self.metrics['Recall'].append(recall)\n",
    "            self.metrics['F1-score'].append(f1)\n",
    "    \n",
    "    def update(self, predictions, true_values):\n",
    "        self.predictions.extend(predictions)\n",
    "        self.true_values.extend(true_values)\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        return self.metrics\n",
    "\n",
    "# 定义数据集类\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, file_path, word2idx, label2idx):\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.word2idx = word2idx\n",
    "        self.label2idx = label2idx\n",
    "        \n",
    "        current_sentence = []\n",
    "        current_labels = []\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    if current_sentence:\n",
    "                        self.sentences.append([word2idx.get(w.lower(), word2idx['<UNK>']) for w in current_sentence])\n",
    "                        self.labels.append([label2idx[l] for l in current_labels])\n",
    "                        current_sentence = []\n",
    "                        current_labels = []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    current_sentence.append(parts[0])\n",
    "                    current_labels.append(parts[-1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "def build_vocab(train_path):\n",
    "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    label2idx = {}\n",
    "    word_freq = defaultdict(int)\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                word = parts[0].lower()\n",
    "                label = parts[-1]\n",
    "                word_freq[word] += 1\n",
    "                if label not in label2idx:\n",
    "                    label2idx[label] = len(label2idx)\n",
    "    \n",
    "    for word, freq in word_freq.items():\n",
    "        if freq > 1 and word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "    \n",
    "    return word2idx, label2idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sentences, labels = zip(*batch)\n",
    "    max_len = len(sentences[0])\n",
    "    \n",
    "    padded_sentences = []\n",
    "    padded_labels = []\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        padding_len = max_len - len(sentence)\n",
    "        padded_sentence = torch.cat([sentence, torch.zeros(padding_len, dtype=torch.long)])\n",
    "        padded_label = torch.cat([label, torch.zeros(padding_len, dtype=torch.long)])\n",
    "        padded_sentences.append(padded_sentence)\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    return torch.stack(padded_sentences), torch.stack(padded_labels)\n",
    "\n",
    "def load_glove_embeddings(word2idx, embed_dim=100):\n",
    "    glove_path = 'glove.6B.100d.txt'\n",
    "    print(\"正在加载GloVe词向量...\")\n",
    "    \n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), embed_dim))\n",
    "    \n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                if word in word2idx:\n",
    "                    vector = np.array(values[1:], dtype='float32')\n",
    "                    embeddings[word2idx[word]] = vector\n",
    "        print(\"GloVe词向量加载完成！\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载词向量时出错: {e}\")\n",
    "        print(\"使用随机初始化的词向量继续...\")\n",
    "    \n",
    "    return torch.FloatTensor(embeddings)\n",
    "\n",
    "# CRF模型定义\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_tags = num_tags\n",
    "        # 使用更小的初始值初始化转移矩阵\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags) * 0.1)\n",
    "        \n",
    "        # 初始化转移矩阵\n",
    "        self.transitions.data[0, :] = -10000  # 从PAD标签转移的代价很高\n",
    "        self.transitions.data[:, 0] = -10000  # 转移到PAD标签的代价很高\n",
    "    \n",
    "    def forward(self, emissions, tags, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
    "        \n",
    "        loss = -self.log_likelihood(emissions, tags, mask)\n",
    "        return loss.mean()\n",
    "    \n",
    "    def decode(self, emissions, mask=None):\n",
    "        \"\"\"使用维特比算法进行解码\"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.size(0), emissions.size(1), dtype=torch.uint8, device=emissions.device)\n",
    "        \n",
    "        batch_size, seq_len, num_tags = emissions.size()\n",
    "        \n",
    "        # 初始化\n",
    "        viterbi = emissions.new_full((batch_size, num_tags), -10000)\n",
    "        viterbi[:, 0] = 0  # 从PAD标签开始\n",
    "        \n",
    "        # 存储最佳路径\n",
    "        backpointers = emissions.new_zeros((batch_size, seq_len, num_tags), dtype=torch.long)\n",
    "        \n",
    "        # 前向传播\n",
    "        for t in range(seq_len):\n",
    "            # 计算当前时间步的得分\n",
    "            emit_score = emissions[:, t].unsqueeze(2)  # [batch_size, num_tags, 1]\n",
    "            trans_score = self.transitions.unsqueeze(0)  # [1, num_tags, num_tags]\n",
    "            \n",
    "            # 计算所有可能的路径得分\n",
    "            next_tag_var = viterbi.unsqueeze(2) + trans_score + emit_score  # [batch_size, num_tags, num_tags]\n",
    "            \n",
    "            # 找到最佳路径\n",
    "            best_tag_scores, best_tag_ids = next_tag_var.max(dim=1)  # [batch_size, num_tags]\n",
    "            \n",
    "            # 应用mask\n",
    "            best_tag_scores = best_tag_scores * mask[:, t].unsqueeze(1)\n",
    "            \n",
    "            # 更新viterbi和backpointers\n",
    "            viterbi = best_tag_scores\n",
    "            backpointers[:, t] = best_tag_ids\n",
    "        \n",
    "        # 回溯找到最佳路径\n",
    "        best_path_scores, best_tag_ids = viterbi.max(dim=1)\n",
    "        best_paths = [best_tag_ids]\n",
    "        \n",
    "        # 从后向前回溯\n",
    "        for t in range(seq_len-1, 0, -1):\n",
    "            best_tag_ids = torch.gather(backpointers[:, t], 1, best_tag_ids.unsqueeze(1)).squeeze(1)\n",
    "            best_paths.insert(0, best_tag_ids)\n",
    "        \n",
    "        # 将路径转换为tensor\n",
    "        best_paths = torch.stack(best_paths, dim=1)\n",
    "        \n",
    "        return best_paths\n",
    "    \n",
    "    def log_likelihood(self, emissions, tags, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
    "        \n",
    "        forward_score = self._forward_alg(emissions, mask)\n",
    "        gold_score = self._score_sentence(emissions, tags, mask)\n",
    "        return gold_score - forward_score\n",
    "    \n",
    "    def _forward_alg(self, emissions, mask):\n",
    "        batch_size, seq_len, num_tags = emissions.size()\n",
    "        \n",
    "        alpha = emissions.new_full((batch_size, num_tags), -10000)\n",
    "        alpha[:, 0] = 0\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            emit_score = emissions[:, t].unsqueeze(2)\n",
    "            trans_score = self.transitions.unsqueeze(0)\n",
    "            next_tag_var = alpha.unsqueeze(2) + trans_score + emit_score\n",
    "            alpha = torch.logsumexp(next_tag_var, dim=1) * mask[:, t].unsqueeze(1)\n",
    "        \n",
    "        return torch.logsumexp(alpha, dim=1)\n",
    "    \n",
    "    def _score_sentence(self, emissions, tags, mask):\n",
    "        batch_size, seq_len, num_tags = emissions.size()\n",
    "        score = emissions.new_zeros(batch_size)\n",
    "        \n",
    "        score += emissions[torch.arange(batch_size), 0, tags[:, 0]] * mask[:, 0]\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            trans_score = self.transitions[tags[:, t-1], tags[:, t]]\n",
    "            emit_score = emissions[torch.arange(batch_size), t, tags[:, t]]\n",
    "            score += (trans_score + emit_score) * mask[:, t]\n",
    "        \n",
    "        return score\n",
    "\n",
    "# BiLSTM-CRF模型定义\n",
    "class BiLSTM_CRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_tags, dropout=0.3, pretrained_embeddings=None):\n",
    "        super(BiLSTM_CRF_NER, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_tags = num_tags\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "            self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim // 2,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_tags)\n",
    "        self.crf = CRF(num_tags)\n",
    "    \n",
    "    def forward(self, x, tags=None, mask=None):\n",
    "        emissions = self._get_emissions(x)\n",
    "        \n",
    "        if self.training and tags is not None:\n",
    "            # 训练模式：计算损失\n",
    "            return self.crf(emissions, tags, mask)\n",
    "        else:\n",
    "            # 评估模式：使用维特比算法解码\n",
    "            return self.crf.decode(emissions, mask)\n",
    "    \n",
    "    def _get_emissions(self, x):\n",
    "        embeds = self.word_embeddings(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return emissions\n",
    "\n",
    "# MEMM模型定义\n",
    "class MEMM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_tags, dropout=0.3, pretrained_embeddings=None):\n",
    "        super(MEMM_NER, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_tags = num_tags\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "            self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        self.tag_embeddings = nn.Embedding(num_tags, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim + embedding_dim,\n",
    "                           hidden_dim // 2,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_tags)\n",
    "    \n",
    "    def forward(self, x, prev_tags=None):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        word_embeds = self.word_embeddings(x)\n",
    "        \n",
    "        if prev_tags is None:\n",
    "            prev_tags = torch.zeros(batch_size, seq_len, dtype=torch.long, device=x.device)\n",
    "        else:\n",
    "            prev_tags = torch.cat([torch.zeros(batch_size, 1, dtype=torch.long, device=x.device),\n",
    "                                 prev_tags[:, :-1]], dim=1)\n",
    "        \n",
    "        tag_embeds = self.tag_embeddings(prev_tags)\n",
    "        combined_embeds = torch.cat([word_embeds, tag_embeds], dim=-1)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(combined_embeds)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return tag_space\n",
    "\n",
    "def beam_search_decode(model, sentence, beam_size=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_size, seq_len = sentence.size()\n",
    "        device = sentence.device\n",
    "        \n",
    "        all_predictions = []\n",
    "        for b in range(batch_size):\n",
    "            mask = (sentence[b] != 0)\n",
    "            \n",
    "            beams = [([], 0.0)]\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                if not mask[t]:\n",
    "                    continue\n",
    "                    \n",
    "                candidates = []\n",
    "                for seq, score in beams:\n",
    "                    prev_tags = torch.zeros(1, seq_len, dtype=torch.long, device=device)\n",
    "                    for i, tag in enumerate(seq):\n",
    "                        prev_tags[0, i] = tag\n",
    "                    \n",
    "                    current_input = sentence[b:b+1]\n",
    "                    outputs = model(current_input, prev_tags)\n",
    "                    probs = F.log_softmax(outputs[0, t], dim=-1)\n",
    "                    \n",
    "                    topk_probs, topk_tags = probs.topk(beam_size)\n",
    "                    \n",
    "                    for prob, tag in zip(topk_probs, topk_tags):\n",
    "                        if tag != 0:\n",
    "                            candidates.append((seq + [tag.item()], score + prob.item()))\n",
    "            \n",
    "                beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        \n",
    "            if beams:\n",
    "                best_seq = beams[0][0]\n",
    "                best_seq.extend([0] * (seq_len - len(best_seq)))\n",
    "                all_predictions.append(best_seq)\n",
    "            else:\n",
    "                all_predictions.append([0] * seq_len)\n",
    "        \n",
    "        return torch.tensor(all_predictions, device=device)\n",
    "\n",
    "def evaluate_with_beam_search(model, data_loader, beam_size=5):\n",
    "    metrics = MetricsHandler(list(range(len(label2idx))))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            predictions = beam_search_decode(model, inputs, beam_size)\n",
    "            \n",
    "            mask = (targets != 0)\n",
    "            predictions = predictions[mask].cpu().numpy()\n",
    "            true_values = targets[mask].cpu().numpy()\n",
    "            \n",
    "            valid_mask = (predictions > 0) & (true_values > 0)\n",
    "            predictions = predictions[valid_mask]\n",
    "            true_values = true_values[valid_mask]\n",
    "            \n",
    "            if len(predictions) > 0:\n",
    "                metrics.collect()\n",
    "                metrics.update(predictions, true_values)\n",
    "    \n",
    "    metrics_dict = metrics.get_metrics()\n",
    "    return metrics_dict['F1-score'][-1] if metrics_dict['F1-score'] else 0.0\n",
    "\n",
    "def evaluate_crf(model, data_loader):\n",
    "    metrics = MetricsHandler(list(range(len(label2idx))))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            mask = (targets != 0)\n",
    "            predictions = model(inputs, mask=mask)\n",
    "            \n",
    "            predictions = predictions[mask].cpu().numpy()\n",
    "            true_values = targets[mask].cpu().numpy()\n",
    "            \n",
    "            valid_mask = (predictions > 0) & (true_values > 0)\n",
    "            predictions = predictions[valid_mask]\n",
    "            true_values = true_values[valid_mask]\n",
    "            \n",
    "            if len(predictions) > 0:\n",
    "                metrics.collect()\n",
    "                metrics.update(predictions, true_values)\n",
    "    \n",
    "    metrics_dict = metrics.get_metrics()\n",
    "    return metrics_dict['F1-score'][-1] if metrics_dict['F1-score'] else 0.0\n",
    "\n",
    "def train_and_evaluate_models():\n",
    "    # 初始化数据\n",
    "    train_path = 'data/train.txt'\n",
    "    dev_path = 'data/dev.txt'\n",
    "    test_path = 'data/test.txt'\n",
    "    \n",
    "    word2idx, label2idx = build_vocab(train_path)\n",
    "    train_dataset = CoNLLDataset(train_path, word2idx, label2idx)\n",
    "    dev_dataset = CoNLLDataset(dev_path, word2idx, label2idx)\n",
    "    test_dataset = CoNLLDataset(test_path, word2idx, label2idx)\n",
    "    \n",
    "    # 加载预训练词嵌入\n",
    "    pretrained_embeddings = load_glove_embeddings(word2idx)\n",
    "    \n",
    "    # 创建DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=256, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    dev_loader = DataLoader(\n",
    "        dev_dataset, \n",
    "        batch_size=256, \n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=256, \n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # 初始化模型参数\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_dim = 100\n",
    "    hidden_dim = 256\n",
    "    num_layers = 2\n",
    "    num_tags = len(label2idx)\n",
    "    dropout = 0.3\n",
    "    \n",
    "    # 初始化模型\n",
    "    memm_model = MEMM_NER(vocab_size, embedding_dim, hidden_dim, num_layers, num_tags, \n",
    "                         dropout=dropout, pretrained_embeddings=pretrained_embeddings)\n",
    "    crf_model = BiLSTM_CRF_NER(vocab_size, embedding_dim, hidden_dim, num_layers, num_tags,\n",
    "                              dropout=dropout, pretrained_embeddings=pretrained_embeddings)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memm_model = memm_model.cuda()\n",
    "        crf_model = crf_model.cuda()\n",
    "    \n",
    "    # 优化器\n",
    "    memm_optimizer = optim.AdamW(memm_model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    crf_optimizer = optim.AdamW(crf_model.parameters(), lr=0.0001, weight_decay=0.001)  # 降低CRF的学习率\n",
    "    \n",
    "    # 学习率调度器\n",
    "    memm_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        memm_optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "    crf_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        crf_optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    # 创建梯度缩放器\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    memm_scaler = GradScaler() if use_amp else None\n",
    "    crf_scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    # 训练循环\n",
    "    num_epochs = 20\n",
    "    best_f1 = {'memm': -1, 'crf': -1}\n",
    "    patience = 5\n",
    "    no_improve_count = {'memm': 0, 'crf': 0}\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # 训练MEMM模型\n",
    "        memm_model.train()\n",
    "        memm_train_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            inputs, targets = batch\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            memm_optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = memm_model(inputs, targets)\n",
    "                    mask = (targets != 0).float()\n",
    "                    loss = criterion(outputs.view(-1, num_tags), targets.view(-1))\n",
    "                    loss = (loss * mask.view(-1)).sum() / mask.sum()\n",
    "                \n",
    "                memm_scaler.scale(loss).backward()\n",
    "                memm_scaler.unscale_(memm_optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(memm_model.parameters(), 1.0)  # 增加梯度裁剪阈值\n",
    "                memm_scaler.step(memm_optimizer)\n",
    "                memm_scaler.update()\n",
    "            else:\n",
    "                outputs = memm_model(inputs, targets)\n",
    "                mask = (targets != 0).float()\n",
    "                loss = criterion(outputs.view(-1, num_tags), targets.view(-1))\n",
    "                loss = (loss * mask.view(-1)).sum() / mask.sum()\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(memm_model.parameters(), 1.0)\n",
    "                memm_optimizer.step()\n",
    "            \n",
    "            memm_train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'MEMM Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # 评估MEMM模型\n",
    "        memm_model.eval()\n",
    "        dev_f1_memm = evaluate_with_beam_search(memm_model, dev_loader)\n",
    "        print(f\"MEMM - Train Loss: {memm_train_loss/len(train_loader):.4f}, Dev F1: {dev_f1_memm:.4f}\")\n",
    "        \n",
    "        # 训练CRF模型\n",
    "        crf_model.train()\n",
    "        crf_train_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            inputs, targets = batch\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            crf_optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    mask = (targets != 0)\n",
    "                    loss = crf_model(inputs, targets, mask)\n",
    "                \n",
    "                crf_scaler.scale(loss).backward()\n",
    "                crf_scaler.unscale_(crf_optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(crf_model.parameters(), 1.0)\n",
    "                crf_scaler.step(crf_optimizer)\n",
    "                crf_scaler.update()\n",
    "            else:\n",
    "                mask = (targets != 0)\n",
    "                loss = crf_model(inputs, targets, mask)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(crf_model.parameters(), 1.0)\n",
    "                crf_optimizer.step()\n",
    "            \n",
    "            crf_train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'CRF Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # 评估CRF模型\n",
    "        crf_model.eval()\n",
    "        dev_f1_crf = evaluate_crf(crf_model, dev_loader)\n",
    "        print(f\"CRF - Train Loss: {crf_train_loss/len(train_loader):.4f}, Dev F1: {dev_f1_crf:.4f}\")\n",
    "        \n",
    "        # 更新学习率\n",
    "        memm_scheduler.step(dev_f1_memm)\n",
    "        crf_scheduler.step(dev_f1_crf)\n",
    "        \n",
    "        print(f\"MEMM LR: {memm_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(f\"CRF LR: {crf_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if dev_f1_memm > best_f1['memm']:\n",
    "            best_f1['memm'] = dev_f1_memm\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': memm_model.state_dict(),\n",
    "                'optimizer_state_dict': memm_optimizer.state_dict(),\n",
    "                'scheduler_state_dict': memm_scheduler.state_dict(),\n",
    "                'best_f1': dev_f1_memm,\n",
    "            }, 'best_memm_model.pt')\n",
    "            no_improve_count['memm'] = 0\n",
    "        else:\n",
    "            no_improve_count['memm'] += 1\n",
    "            \n",
    "        if dev_f1_crf > best_f1['crf']:\n",
    "            best_f1['crf'] = dev_f1_crf\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': crf_model.state_dict(),\n",
    "                'optimizer_state_dict': crf_optimizer.state_dict(),\n",
    "                'scheduler_state_dict': crf_scheduler.state_dict(),\n",
    "                'best_f1': dev_f1_crf,\n",
    "            }, 'best_crf_model.pt')\n",
    "            no_improve_count['crf'] = 0\n",
    "        else:\n",
    "            no_improve_count['crf'] += 1\n",
    "        \n",
    "        # 早停检查\n",
    "        if min(no_improve_count.values()) >= patience:\n",
    "            print(f\"早停：{patience}个epoch未改善\")\n",
    "            break\n",
    "    \n",
    "    # 最终测试\n",
    "    print(\"\\n最终测试结果：\")\n",
    "    \n",
    "    # 测试MEMM模型\n",
    "    checkpoint = torch.load('best_memm_model.pt')\n",
    "    memm_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_f1_memm = evaluate_with_beam_search(memm_model, test_loader)\n",
    "    print(f\"MEMM Test F1: {test_f1_memm:.4f}\")\n",
    "    \n",
    "    # 测试CRF模型\n",
    "    checkpoint = torch.load('best_crf_model.pt')\n",
    "    crf_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_f1_crf = evaluate_crf(crf_model, test_loader)\n",
    "    print(f\"CRF Test F1: {test_f1_crf:.4f}\")\n",
    "\n",
    "# 运行训练和评估\n",
    "train_and_evaluate_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
