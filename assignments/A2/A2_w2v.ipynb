{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 2. Word2vec Implementation \n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "Train a word2vec model using the **skip-gram** architecture and **negative sampling**.\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from *Lab 4 (part 2): Data preparation for implementing word2vec*. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from utils import CorpusReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "The corpus data is in `lunyu_20chapters.txt`. Use the `CorpusReader` class in `utils.py` to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n",
      "Total words in text: 0\n",
      "Vocabulary size: 1352\n"
     ]
    }
   ],
   "source": [
    "# Read raw data in lunyu_20chapters.txt\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "corpus = CorpusReader(inputFileName=\"lunyu_20chapters.txt\", min_count=1)\n",
    "\n",
    "# 读取原始文本并分词\n",
    "with open(\"lunyu_20chapters.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    words = text.split()  # 简单分词\n",
    "\n",
    "# 过滤掉不在词典中的词\n",
    "words = [word for word in words if word in corpus.word2id]\n",
    "print(f\"Total words in text: {len(words)}\")\n",
    "print(f\"Vocabulary size: {corpus.vocab_size}\")\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the code from lab with necessary modifications\n",
    "\n",
    "def generate_data(words: List[str], window_size: int, k: int, corpus: CorpusReader):\n",
    "    \"\"\" Generate the training data for word2vec skip-gram model\n",
    "    Args:\n",
    "        text: the input text\n",
    "        window_size: the size of the context window\n",
    "        k: the number of negative samples\n",
    "        corpus: the corpus object, providing utilities such as word2id, getNegatives, etc.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    data = []\n",
    "    # 使用词典大小作为负采样的范围\n",
    "    vocab_size = len(corpus.word2id)\n",
    "    \n",
    "    for i, center_word in enumerate(corpus.word2id):\n",
    "        \n",
    "        if center_word not in corpus.word2id:\n",
    "            continue\n",
    "            \n",
    "        center_id = corpus.word2id[center_word]\n",
    "        # 获取上下文窗口\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(words), i + window_size + 1)\n",
    "        \n",
    "        # 对窗口内的每个上下文词\n",
    "        for j in range(window_start, window_end):\n",
    "            if i != j:  # 跳过中心词自身\n",
    "                context_word = words[j]\n",
    "                if context_word not in corpus.word2id:\n",
    "                    continue\n",
    "                    \n",
    "                outside_id = corpus.word2id[context_word]\n",
    "                # 获取负样本\n",
    "                negative_ids = corpus.getNegatives(center_id, k)\n",
    "                # 确保negative_ids是numpy数组\n",
    "                negative_ids = np.array(negative_ids, dtype=np.int64)\n",
    "                \n",
    "                data.append((center_id, outside_id, negative_ids))\n",
    "    \n",
    "    return data\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "def batchify(data: List, batch_size: int):\n",
    "    \"\"\" Group a stream into batches and yield them as torch tensors.\n",
    "    Args:\n",
    "        data: a list of tuples\n",
    "        batch_size: the batch size \n",
    "    Yields:\n",
    "        a tuple of three torch tensors: center, outside, negative\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data) # data should be long enough\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        if i > len(data) - batch_size: # if the last batch is smaller than batch_size, pad it with the first few data\n",
    "            batch = batch + data[:i + batch_size - len(data)]\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # 预先创建numpy数组，然后一次性转换为tensor\n",
    "        center_words = np.array([item[0] for item in batch])\n",
    "        outside_words = np.array([item[1] for item in batch])\n",
    "        # 对于negative samples，需要特别处理因为它是二维的\n",
    "        negative_samples = np.array([item[2] for item in batch])\n",
    "        \n",
    "        # 转换为tensor\n",
    "        center_tensor = torch.LongTensor(center_words)\n",
    "        outside_tensor = torch.LongTensor(outside_words)\n",
    "        negative_tensor = torch.LongTensor(negative_samples)\n",
    "        \n",
    "        yield center_tensor, outside_tensor, negative_tensor\n",
    "        ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_v = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.emb_u = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_size # some experience passed down from generation to generation\n",
    "        nn.init.uniform_(self.emb_v.weight.data, -initrange, initrange) # same outcome as self.emb_v.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.constant_(self.emb_u.weight.data, 0) # same outcome as self.emb_u.weight.data.zero_()\n",
    "\n",
    "    def forward(self, center, outside, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center: the center word indices (B, )\n",
    "            outside: the outside word indices (B, )\n",
    "            negative: the negative word indices (B, k)\n",
    "        \"\"\"\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute positive score\n",
    "        pos_score = torch.sum(v_c * u_o, dim=1)  # (B,)\n",
    "        pos_score = torch.clamp(pos_score, min=-10, max=10)  # 防止数值溢出\n",
    "        pos_loss = F.logsigmoid(pos_score)  # (B,)\n",
    "        \n",
    "        # Compute negative score\n",
    "        # reshape v_c to (B, 1, emb_size) for broadcasting\n",
    "        neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze()  # (B, k)\n",
    "        neg_score = torch.clamp(neg_score, min=-10, max=10)  # 防止数值溢出\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)  # (B,)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        # Hint: torch.clamp the input to F.logsigmoid to avoid numerical underflow/overflow\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.emb_v.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_size))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n"
     ]
    }
   ],
   "source": [
    "def train(model, words, corpus, batch_size, window_size, k, optimizer, scheduler, \n",
    "          epochs, device='cpu', print_interval=100):\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    try:\n",
    "        # 生成训练数据\n",
    "        print(\"生成训练数据...\")\n",
    "        train_data = generate_data(words, window_size, k, corpus)\n",
    "        print(f\"生成了 {len(train_data)} 个训练样本\")\n",
    "        \n",
    "        if len(train_data) == 0:\n",
    "            raise ValueError(\"没有生成任何训练数据！\")\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            # 打乱训练数据\n",
    "            np.random.shuffle(train_data)\n",
    "            \n",
    "            # 创建进度条\n",
    "            total_batches = len(train_data) // batch_size\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, 总批次数: {total_batches}\")\n",
    "            \n",
    "            for center, outside, negative in batchify(train_data, batch_size):\n",
    "                try:\n",
    "                    # 移动数据到设备\n",
    "                    center = center.to(device)\n",
    "                    outside = outside.to(device)\n",
    "                    negative = negative.to(device)\n",
    "                    \n",
    "                    # 前向传播\n",
    "                    model.zero_grad()\n",
    "                    loss = model(center, outside, negative)\n",
    "                    \n",
    "                    # 反向传播\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    if batch_count % print_interval == 0:\n",
    "                        avg_loss = total_loss / print_interval\n",
    "                        print(f\"Epoch {epoch+1}, Batch {batch_count}/{total_batches}, \"\n",
    "                              f\"Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "                        losses.append(avg_loss)\n",
    "                        total_loss = 0\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"批次训练出错: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1} 完成\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"训练过程出错: {e}\")\n",
    "        raise\n",
    "        \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Suggested hyperparameters\n",
    "initial_lr = 0.025\n",
    "batch_size = 16\n",
    "emb_size = 50\n",
    "window_size = 5\n",
    "k = 10 # the number of negative samples, change with your own choice for better embedding performance\n",
    "min_count = 1 # because our data is small. If min_count > 1, you should filter out those unknown words from the data in train() function\n",
    "# optimizer = torch.optim.Adam() # or torch.optim.SparseAdam()\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR() # or torch.optim.lr_scheduler.StepLR()\n",
    "\n",
    "# Initialize the corpus and model\n",
    "corpus = CorpusReader('lunyu_20chapters.txt', min_count)\n",
    "vocab_size = corpus.vocab_size\n",
    "model = SkipGram(vocab_size, emb_size)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# 使用修改后的函数进行训练\n",
    "epochs = 10\n",
    "losses = train(model, corpus.id2word, corpus, batch_size, window_size, k, optimizer, scheduler, \n",
    "              epochs, device=device, print_interval=100)\n",
    "\n",
    "\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Batch (x100)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig('loss_curve.png')\n",
    "plt.show()\n",
    "\n",
    "### Hints: ###\n",
    "# - If you have cuda-supported GPUs, you can run the training faster by\n",
    "#   `device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")`\n",
    "#   `model.cuda()`\n",
    "#   You also need to move all tensor data to the same device\n",
    "# - If you find Inf or NaN in the loss, you can try to clip the gradient usning `torch.nn.utils.clip_grad_norm_`\n",
    "# - Remember to save the embeddings when training is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Embeddings\n",
    "\n",
    "\n",
    "Save the embeddings into a `gensim` compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'embeddings.txt'\n",
    "\n",
    "weights = model.emb_v.detech().cpu().data.numpy()\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(f\"{vocab_size} {emb_size}\\n\")  # First line: vocab size and vector dimension\n",
    "    for idx, vector in enumerate(weights):\n",
    "        vector_str = \" \".join(map(str, vector))\n",
    "        f.write(f\"{model.id2word[idx]} {vector_str}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot and Compare Embeddings\n",
    "\n",
    "Use `sklearn.decomposition.TruncatedSVD` to reduce the dimensionality of the obtained embeddings to 2 and plot the selected words in 2D space.\n",
    "\n",
    "*Hint*:\n",
    "- Obtain the embeddings into a numpy array by `model.emb_v.cpu().data.numpy()`\n",
    "- The word2id dictionary is in `model.word2id`\n",
    "- If you are trying to load from a saved embedding file, you can use the APIs from `gensim`.\n",
    "  - For exmaple, `model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')`\n",
    "  - Check out the documentation for more details: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "### YOUR CODE HERE ###\n",
    "pass\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD\n",
    "### YOUR CODE HERE ###\n",
    "pass\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the following words or other words you are interested in\n",
    "# You better pick those words that look different in the 2D space compared with the LSA vectors\n",
    "words = ['学', '习', '曰', '子', '人', '仁']\n",
    "words_pinyin = ['xue', 'xi', 'yue', 'zi', 'ren1', 'ren2']\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "pass\n",
    "### END YOUR CODE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
